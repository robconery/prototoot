# Lab 1: ‚òÅÔ∏è S3 Integration & Data Pipeline

Transform your local data pipeline into a cloud-native architecture! Build streaming data pipelines that move your Chinook customer data from SQLite through Bufstream to AWS S3.

## üéØ Objectives
- Integrate with AWS S3 for scalable data storage
- Implement partitioned data storage strategies
- Build streaming pipelines with automatic retries
- Handle large file uploads and lifecycle management

## üöÄ Setup

### 1. AWS Configuration
First, ensure your AWS credentials are configured:

```bash
# Check if AWS CLI is configured
aws configure list

# If not configured, set it up
aws configure
# AWS Access Key ID: [Your access key]
# AWS Secret Access Key: [Your secret key]
# Default region name: us-west-2
# Default output format: json

# Test S3 access
aws s3 ls
```

### 2. Create S3 Bucket for Chinook Data
```bash
# Create bucket (replace with your unique name)
aws s3 mb s3://chinook-data-pipeline-$(date +%s)

# Enable versioning
aws s3api put-bucket-versioning \
  --bucket big-machine-data-pipeline-$(date +%s) \
  --versioning-configuration Status=Enabled
```

### 3. Install Required Go Dependencies
```bash
cd /path/to/your/workshop/day06-cloud-day/lab01

# Initialize Go module if not exists
go mod init big-machine-s3-pipeline

# Install AWS SDK
go get github.com/aws/aws-sdk-go-v2
go get github.com/aws/aws-sdk-go-v2/config
go get github.com/aws/aws-sdk-go-v2/service/s3
go get github.com/aws/aws-sdk-go-v2/feature/s3/manager

# Install other dependencies
go get google.golang.org/protobuf/proto
go get github.com/mattn/go-sqlite3
```

## üèóÔ∏è Implementation

### 1. S3 Configuration (`internal/s3/config.go`)
```go
package s3

import (
	"context"
	"fmt"
	"time"

	"github.com/aws/aws-sdk-go-v2/aws"
	"github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/service/s3"
)

type Config struct {
	BucketName string
	Region     string
	Prefix     string
}

type S3Client struct {
	client *s3.Client
	config Config
}

func NewS3Client(cfg Config) (*S3Client, error) {
	awsConfig, err := config.LoadDefaultConfig(context.TODO(),
		config.WithRegion(cfg.Region),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to load AWS config: %w", err)
	}

	return &S3Client{
		client: s3.NewFromConfig(awsConfig),
		config: cfg,
	}, nil
}

func (s *S3Client) HealthCheck(ctx context.Context) error {
	_, err := s.client.HeadBucket(ctx, &s3.HeadBucketInput{
		Bucket: aws.String(s.config.BucketName),
	})
	if err != nil {
		return fmt.Errorf("S3 bucket health check failed: %w", err)
	}
	return nil
}
```

### 2. Data Partitioning Strategy (`internal/s3/partitioner.go`)
```go
package s3

import (
	"fmt"
	"time"
)

type PartitionStrategy interface {
	GenerateKey(entityType, entityID string, timestamp time.Time) string
}

type DateHourPartitioner struct {
	prefix string
}

func NewDateHourPartitioner(prefix string) *DateHourPartitioner {
	return &DateHourPartitioner{prefix: prefix}
}

func (p *DateHourPartitioner) GenerateKey(entityType, entityID string, timestamp time.Time) string {
	// Generate key: prefix/year=2024/month=01/day=15/hour=14/entity_type/entity_id_timestamp.pb
	return fmt.Sprintf("%s/year=%d/month=%02d/day=%02d/hour=%02d/%s/%s_%d.pb",
		p.prefix,
		timestamp.Year(),
		timestamp.Month(),
		timestamp.Day(),
		timestamp.Hour(),
		entityType,
		entityID,
		timestamp.Unix(),
	)
}

// Business metrics partitioning for analytics
type BusinessPartitioner struct {
	prefix string
}

func NewBusinessPartitioner(prefix string) *BusinessPartitioner {
	return &BusinessPartitioner{prefix: prefix}
}

func (p *BusinessPartitioner) GenerateKey(entityType, entityID string, timestamp time.Time) string {
	// Generate key optimized for business queries
	return fmt.Sprintf("%s/%s/date=%s/%s_%d.pb",
		p.prefix,
		entityType,
		timestamp.Format("2006-01-02"),
		entityID,
		timestamp.Unix(),
	)
}
```

### 3. S3 Uploader with Retry Logic (`internal/s3/uploader.go`)
```go
package s3

import (
	"bytes"
	"context"
	"fmt"
	"io"
	"log"
	"time"

	"github.com/aws/aws-sdk-go-v2/aws"
	"github.com/aws/aws-sdk-go-v2/feature/s3/manager"
	"github.com/aws/aws-sdk-go-v2/service/s3"
)

type UploadResult struct {
	Key       string
	ETag      string
	VersionID string
	Size      int64
}

type UploaderConfig struct {
	MaxRetries    int
	RetryDelay    time.Duration
	PartSize      int64 // For multipart uploads
	Concurrency   int
}

type Uploader struct {
	s3Client    *S3Client
	uploader    *manager.Uploader
	config      UploaderConfig
	partitioner PartitionStrategy
}

func NewUploader(s3Client *S3Client, partitioner PartitionStrategy, config UploaderConfig) *Uploader {
	uploader := manager.NewUploader(s3Client.client, func(u *manager.Uploader) {
		u.PartSize = config.PartSize
		u.Concurrency = config.Concurrency
	})

	return &Uploader{
		s3Client:    s3Client,
		uploader:    uploader,
		config:      config,
		partitioner: partitioner,
	}
}

func (u *Uploader) UploadData(ctx context.Context, entityType, entityID string, data []byte, metadata map[string]string) (*UploadResult, error) {
	key := u.partitioner.GenerateKey(entityType, entityID, time.Now())
	
	// Add default metadata
	if metadata == nil {
		metadata = make(map[string]string)
	}
	metadata["entity-type"] = entityType
	metadata["entity-id"] = entityID
	metadata["upload-timestamp"] = time.Now().UTC().Format(time.RFC3339)
	metadata["content-type"] = "application/x-protobuf"

	var result *UploadResult
	var err error

	// Retry logic
	for attempt := 0; attempt <= u.config.MaxRetries; attempt++ {
		if attempt > 0 {
			log.Printf("Retrying upload attempt %d for key: %s", attempt, key)
			time.Sleep(u.config.RetryDelay * time.Duration(attempt))
		}

		result, err = u.attemptUpload(ctx, key, data, metadata)
		if err == nil {
			break
		}

		log.Printf("Upload attempt %d failed for key %s: %v", attempt, key, err)
	}

	return result, err
}

func (u *Uploader) attemptUpload(ctx context.Context, key string, data []byte, metadata map[string]string) (*UploadResult, error) {
	reader := bytes.NewReader(data)

	uploadInput := &s3.PutObjectInput{
		Bucket:      aws.String(u.s3Client.config.BucketName),
		Key:         aws.String(key),
		Body:        reader,
		ContentType: aws.String("application/x-protobuf"),
		Metadata:    metadata,
	}

	output, err := u.uploader.Upload(ctx, uploadInput)
	if err != nil {
		return nil, fmt.Errorf("failed to upload to S3: %w", err)
	}

	return &UploadResult{
		Key:       key,
		ETag:      aws.ToString(output.ETag),
		VersionID: aws.ToString(output.VersionID),
		Size:      int64(len(data)),
	}, nil
}

// Batch upload for efficiency
func (u *Uploader) UploadBatch(ctx context.Context, items []UploadItem) ([]*UploadResult, error) {
	results := make([]*UploadResult, len(items))
	errors := make([]error, len(items))

	// Use semaphore to limit concurrent uploads
	semaphore := make(chan struct{}, u.config.Concurrency)

	for i, item := range items {
		go func(index int, uploadItem UploadItem) {
			semaphore <- struct{}{} // Acquire
			defer func() { <-semaphore }() // Release

			result, err := u.UploadData(ctx, uploadItem.EntityType, uploadItem.EntityID, uploadItem.Data, uploadItem.Metadata)
			results[index] = result
			errors[index] = err
		}(i, item)
	}

	// Wait for all uploads to complete
	for i := 0; i < u.config.Concurrency; i++ {
		semaphore <- struct{}{}
	}

	// Check for errors
	var failedUploads []error
	for _, err := range errors {
		if err != nil {
			failedUploads = append(failedUploads, err)
		}
	}

	if len(failedUploads) > 0 {
		return results, fmt.Errorf("batch upload had %d failures", len(failedUploads))
	}

	return results, nil
}

type UploadItem struct {
	EntityType string
	EntityID   string
	Data       []byte
	Metadata   map[string]string
}
```

### 4. Streaming Pipeline (`cmd/s3-pipeline/main.go`)
```go
package main

import (
	"context"
	"database/sql"
	"encoding/json"
	"flag"
	"fmt"
	"log"
	"os"
	"os/signal"
	"syscall"
	"time"

	_ "github.com/mattn/go-sqlite3"
	"google.golang.org/protobuf/proto"

	"big-machine-s3-pipeline/internal/s3"
	pb "big-machine-s3-pipeline/proto"
)

type Config struct {
	DatabaseURL  string
	S3Bucket     string
	S3Region     string
	S3Prefix     string
	BatchSize    int
	PollInterval time.Duration
}

type Pipeline struct {
	db       *sql.DB
	uploader *s3.Uploader
	config   Config
}

func main() {
	var configFile = flag.String("config", "config.json", "Configuration file path")
	flag.Parse()

	config, err := loadConfig(*configFile)
	if err != nil {
		log.Fatalf("Failed to load config: %v", err)
	}

	pipeline, err := NewPipeline(config)
	if err != nil {
		log.Fatalf("Failed to create pipeline: %v", err)
	}
	defer pipeline.Close()

	// Graceful shutdown
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	go func() {
		sigChan := make(chan os.Signal, 1)
		signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
		<-sigChan
		log.Println("Shutdown signal received")
		cancel()
	}()

	log.Println("Starting Chinook S3 Pipeline...")
	if err := pipeline.Run(ctx); err != nil {
		log.Fatalf("Pipeline failed: %v", err)
	}
	log.Println("Pipeline shutdown complete")
}

func NewPipeline(config Config) (*Pipeline, error) {
	// Connect to database
	db, err := sql.Open("sqlite3", config.DatabaseURL)
	if err != nil {
		return nil, fmt.Errorf("failed to connect to database: %w", err)
	}

	if err := db.Ping(); err != nil {
		return nil, fmt.Errorf("failed to ping database: %w", err)
	}

	// Initialize S3 client
	s3Config := s3.Config{
		BucketName: config.S3Bucket,
		Region:     config.S3Region,
		Prefix:     config.S3Prefix,
	}

	s3Client, err := s3.NewS3Client(s3Config)
	if err != nil {
		return nil, fmt.Errorf("failed to create S3 client: %w", err)
	}

	// Health check S3
	if err := s3Client.HealthCheck(context.Background()); err != nil {
		return nil, fmt.Errorf("S3 health check failed: %w", err)
	}

	// Create uploader with business partitioning
	partitioner := s3.NewBusinessPartitioner(config.S3Prefix)
	uploaderConfig := s3.UploaderConfig{
		MaxRetries:  3,
		RetryDelay:  time.Second * 2,
		PartSize:    64 * 1024 * 1024, // 64MB parts
		Concurrency: 5,
	}
	uploader := s3.NewUploader(s3Client, partitioner, uploaderConfig)

	return &Pipeline{
		db:       db,
		uploader: uploader,
		config:   config,
	}, nil
}

func (p *Pipeline) Run(ctx context.Context) error {
	ticker := time.NewTicker(p.config.PollInterval)
	defer ticker.Stop()

	lastProcessedID := 0

	for {
		select {
		case <-ctx.Done():
			log.Println("Pipeline stopping...")
			return nil
		case <-ticker.C:
			processed, err := p.processCustomers(ctx, lastProcessedID)
			if err != nil {
				log.Printf("Error processing customers: %v", err)
				continue
			}
			if processed > 0 {
				lastProcessedID += processed
				log.Printf("Processed %d customers, last ID: %d", processed, lastProcessedID)
			}
		}
	}
}

func (p *Pipeline) processCustomers(ctx context.Context, lastID int) (int, error) {
	query := `
		SELECT id, name, email, phone, status, created_at, updated_at 
		FROM customers 
		WHERE id > ? 
		ORDER BY id 
		LIMIT ?
	`

	rows, err := p.db.QueryContext(ctx, query, lastID, p.config.BatchSize)
	if err != nil {
		return 0, fmt.Errorf("failed to query customers: %w", err)
	}
	defer rows.Close()

	var uploadItems []s3.UploadItem
	processed := 0

	for rows.Next() {
		var customer pb.Customer
		var createdAt, updatedAt time.Time

		err := rows.Scan(
			&customer.Id,
			&customer.Name,
			&customer.Email,
			&customer.Phone,
			&customer.Status,
			&createdAt,
			&updatedAt,
		)
		if err != nil {
			log.Printf("Failed to scan customer row: %v", err)
			continue
		}

		// Convert timestamps
		customer.CreatedAt = createdAt.Unix()
		customer.UpdatedAt = updatedAt.Unix()

		// Serialize protobuf
		data, err := proto.Marshal(&customer)
		if err != nil {
			log.Printf("Failed to marshal customer %d: %v", customer.Id, err)
			continue
		}

		// Add to upload batch
		uploadItems = append(uploadItems, s3.UploadItem{
			EntityType: "customer",
			EntityID:   fmt.Sprintf("%d", customer.Id),
			Data:       data,
			Metadata: map[string]string{
				"customer-status": customer.Status,
				"record-version":  "1.0",
			},
		})

		processed++
	}

	if err := rows.Err(); err != nil {
		return processed, fmt.Errorf("error iterating customer rows: %w", err)
	}

	// Upload batch to S3
	if len(uploadItems) > 0 {
		results, err := p.uploader.UploadBatch(ctx, uploadItems)
		if err != nil {
			return processed, fmt.Errorf("failed to upload batch: %w", err)
		}

		log.Printf("Successfully uploaded %d customers to S3", len(results))
	}

	return processed, nil
}

func (p *Pipeline) Close() error {
	return p.db.Close()
}

func loadConfig(filename string) (Config, error) {
	var config Config

	file, err := os.Open(filename)
	if err != nil {
		return config, err
	}
	defer file.Close()

	decoder := json.NewDecoder(file)
	err = decoder.Decode(&config)
	return config, err
}
```

### 5. Configuration File (`config.json`)
```json
{
  "DatabaseURL": "../resources/chinook.db",
  "S3Bucket": "big-machine-data-pipeline-123456789",
  "S3Region": "us-west-2", 
  "S3Prefix": "customer-data",
  "BatchSize": 100,
  "PollInterval": "30s"
}
```

### 6. Protobuf Schema (`proto/customer.proto`)
```protobuf
syntax = "proto3";

package chinook;

option go_package = "big-machine-s3-pipeline/proto";

message Customer {
  int64 id = 1;
  string name = 2;
  string email = 3;
  string phone = 4;
  string status = 5;
  int64 created_at = 6;
  int64 updated_at = 7;
}
```

## üß™ Testing & Verification

### 1. Generate Test Data
```sql
-- Add some test customers
INSERT INTO customers (name, email, phone, status, created_at, updated_at) VALUES
('Alice Johnson', 'alice@example.com', '555-0101', 'active', NOW(), NOW()),
('Bob Smith', 'bob@example.com', '555-0102', 'inactive', NOW(), NOW()),
('Carol White', 'carol@example.com', '555-0103', 'active', NOW(), NOW());
```

### 2. Build and Run
```bash
# Generate protobuf code
protoc --go_out=. --go_opt=paths=source_relative proto/customer.proto

# Build the pipeline
go build -o bin/s3-pipeline cmd/s3-pipeline/main.go

# Run the pipeline
./bin/s3-pipeline -config=config.json
```

### 3. Verify S3 Data
```bash
# List uploaded files
aws s3 ls s3://your-bucket-name/customer-data/ --recursive

# Download and inspect a file
aws s3 cp s3://your-bucket-name/customer-data/customer/date=2024-01-15/1_1642248123.pb ./test.pb

# Use protoc to decode (optional)
protoc --decode=chinook.Customer proto/customer.proto < test.pb
```

### 4. Monitor Pipeline Performance
```go
// Add metrics collection (internal/metrics/collector.go)
package metrics

import (
	"sync"
	"time"
)

type Metrics struct {
	mu                    sync.RWMutex
	CustomersProcessed    int64
	UploadedBytes         int64
	UploadErrors          int64
	AverageUploadTime     time.Duration
	LastUploadTime        time.Time
}

func (m *Metrics) RecordUpload(bytes int64, duration time.Duration) {
	m.mu.Lock()
	defer m.mu.Unlock()
	
	m.CustomersProcessed++
	m.UploadedBytes += bytes
	m.AverageUploadTime = (m.AverageUploadTime + duration) / 2
	m.LastUploadTime = time.Now()
}

func (m *Metrics) RecordError() {
	m.mu.Lock()
	defer m.mu.Unlock()
	m.UploadErrors++
}

func (m *Metrics) GetStats() map[string]interface{} {
	m.mu.RLock()
	defer m.mu.RUnlock()
	
	return map[string]interface{}{
		"customers_processed": m.CustomersProcessed,
		"uploaded_bytes":      m.UploadedBytes,
		"upload_errors":       m.UploadErrors,
		"avg_upload_time":     m.AverageUploadTime,
		"last_upload":         m.LastUploadTime,
	}
}
```

## ‚úÖ Victory Conditions

- [ ] S3 bucket created and accessible
- [ ] Data pipeline streams customer records to S3 with proper partitioning
- [ ] Failed uploads are retried automatically
- [ ] Uploaded data maintains protobuf format integrity
- [ ] Pipeline handles graceful shutdown
- [ ] Metrics show successful data transfer rates

## üîÑ Extension Ideas

1. **Lifecycle Management**: Implement S3 lifecycle policies for archiving old data
2. **Compression**: Add gzip compression before upload
3. **Deduplication**: Prevent duplicate uploads of unchanged records
4. **Error Recovery**: Store failed uploads in DLQ for manual processing

---

**Next**: Head to `lab02/` to containerize this pipeline! üê≥