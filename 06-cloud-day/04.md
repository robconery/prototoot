# Lab 4: ðŸ“Š Production Observability

Build comprehensive observability into your pipeline! Implement Prometheus metrics, distributed tracing with Jaeger, structured logging, and alerting.

## ðŸŽ¯ Objectives
- Implement Prometheus metrics collection
- Set up distributed tracing with Jaeger
- Configure structured logging with log aggregation
- Create alerting rules and runbooks

## ðŸ“Š Observability Stack

### 1. Prometheus Metrics Integration

#### Enhanced Metrics Collector (`internal/metrics/prometheus.go`)
```go
package metrics

import (
	"net/http"
	"time"

	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promhttp"
)

type PrometheusCollector struct {
	customersProcessed *prometheus.CounterVec
	bytesUploaded     *prometheus.CounterVec
	processingDuration *prometheus.HistogramVec
	uploadDuration    *prometheus.HistogramVec
	errors            *prometheus.CounterVec
	activeBatches     prometheus.Gauge
	queueDepth        prometheus.Gauge
	lastSuccessTime   prometheus.Gauge
}

func NewPrometheusCollector() *PrometheusCollector {
	collector := &PrometheusCollector{
		customersProcessed: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "pipeline_customers_processed_total",
				Help: "Total number of customers processed",
			},
			[]string{"status", "batch_size"},
		),
		bytesUploaded: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "pipeline_bytes_uploaded_total",
				Help: "Total bytes uploaded to S3",
			},
			[]string{"entity_type", "compression"},
		),
		processingDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "pipeline_processing_duration_seconds",
				Help:    "Time spent processing customer batches",
				Buckets: prometheus.DefBuckets,
			},
			[]string{"operation", "batch_size"},
		),
		uploadDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "pipeline_upload_duration_seconds",
				Help:    "Time spent uploading to S3",
				Buckets: []float64{0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0},
			},
			[]string{"bucket", "region"},
		),
		errors: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "pipeline_errors_total",
				Help: "Total number of errors by type",
			},
			[]string{"type", "severity", "component"},
		),
		activeBatches: prometheus.NewGauge(
			prometheus.GaugeOpts{
				Name: "pipeline_active_batches",
				Help: "Number of batches currently being processed",
			},
		),
		queueDepth: prometheus.NewGauge(
			prometheus.GaugeOpts{
				Name: "pipeline_queue_depth",
				Help: "Current depth of processing queue",
			},
		),
		lastSuccessTime: prometheus.NewGauge(
			prometheus.GaugeOpts{
				Name: "pipeline_last_success_timestamp",
				Help: "Timestamp of last successful processing",
			},
		),
	}

	// Register metrics
	prometheus.MustRegister(
		collector.customersProcessed,
		collector.bytesUploaded,
		collector.processingDuration,
		collector.uploadDuration,
		collector.errors,
		collector.activeBatches,
		collector.queueDepth,
		collector.lastSuccessTime,
	)

	return collector
}

func (c *PrometheusCollector) RecordCustomersProcessed(count int, batchSize string, status string) {
	c.customersProcessed.WithLabelValues(status, batchSize).Add(float64(count))
}

func (c *PrometheusCollector) RecordBytesUploaded(bytes int64, entityType, compression string) {
	c.bytesUploaded.WithLabelValues(entityType, compression).Add(float64(bytes))
}

func (c *PrometheusCollector) RecordProcessingDuration(duration time.Duration, operation, batchSize string) {
	c.processingDuration.WithLabelValues(operation, batchSize).Observe(duration.Seconds())
}

func (c *PrometheusCollector) RecordUploadDuration(duration time.Duration, bucket, region string) {
	c.uploadDuration.WithLabelValues(bucket, region).Observe(duration.Seconds())
}

func (c *PrometheusCollector) RecordError(errorType, severity, component string) {
	c.errors.WithLabelValues(errorType, severity, component).Inc()
}

func (c *PrometheusCollector) SetActiveBatches(count float64) {
	c.activeBatches.Set(count)
}

func (c *PrometheusCollector) SetQueueDepth(depth float64) {
	c.queueDepth.Set(depth)
}

func (c *PrometheusCollector) RecordSuccess() {
	c.lastSuccessTime.SetToCurrentTime()
}

func (c *PrometheusCollector) Handler() http.Handler {
	return promhttp.Handler()
}
```

### 2. Distributed Tracing with Jaeger

#### Tracing Setup (`internal/tracing/jaeger.go`)
```go
package tracing

import (
	"context"
	"fmt"
	"io"
	"log"

	"github.com/opentracing/opentracing-go"
	"github.com/opentracing/opentracing-go/ext"
	"github.com/uber/jaeger-client-go"
	"github.com/uber/jaeger-client-go/config"
)

type TracingConfig struct {
	ServiceName string
	JaegerURL   string
	SampleRate  float64
}

func InitTracing(cfg TracingConfig) (opentracing.Tracer, io.Closer, error) {
	tracingConfig := config.Configuration{
		ServiceName: cfg.ServiceName,
		Sampler: &config.SamplerConfig{
			Type:  jaeger.SamplerTypeConst,
			Param: cfg.SampleRate,
		},
		Reporter: &config.ReporterConfig{
			LogSpans:           true,
			LocalAgentHostPort: cfg.JaegerURL,
		},
	}

	tracer, closer, err := tracingConfig.NewTracer()
	if err != nil {
		return nil, nil, fmt.Errorf("failed to create tracer: %w", err)
	}

	opentracing.SetGlobalTracer(tracer)
	return tracer, closer, nil
}

func StartSpan(ctx context.Context, operationName string) (opentracing.Span, context.Context) {
	span, ctx := opentracing.StartSpanFromContext(ctx, operationName)
	return span, ctx
}

func StartSpanWithTags(ctx context.Context, operationName string, tags map[string]interface{}) (opentracing.Span, context.Context) {
	span, ctx := opentracing.StartSpanFromContext(ctx, operationName)
	for key, value := range tags {
		span.SetTag(key, value)
	}
	return span, ctx
}

func RecordError(span opentracing.Span, err error) {
	if err != nil {
		ext.Error.Set(span, true)
		span.LogFields(
			jaeger.Error(err),
		)
	}
}

func FinishSpan(span opentracing.Span) {
	span.Finish()
}

// Database tracing wrapper
func TraceDBQuery(ctx context.Context, query string, args ...interface{}) (opentracing.Span, context.Context) {
	span, ctx := StartSpanWithTags(ctx, "db.query", map[string]interface{}{
		"db.type":      "postgresql",
		"db.statement": query,
		"component":    "database",
	})
	return span, ctx
}

// S3 tracing wrapper
func TraceS3Upload(ctx context.Context, bucket, key string, size int64) (opentracing.Span, context.Context) {
	span, ctx := StartSpanWithTags(ctx, "s3.upload", map[string]interface{}{
		"aws.s3.bucket": bucket,
		"aws.s3.key":    key,
		"aws.s3.size":   size,
		"component":     "s3",
	})
	return span, ctx
}
```

### 3. Structured Logging

#### Enhanced Logger (`internal/logging/logger.go`)
```go
package logging

import (
	"context"
	"os"
	"time"

	"github.com/opentracing/opentracing-go"
	"github.com/sirupsen/logrus"
	"github.com/uber/jaeger-client-go"
)

type Logger struct {
	*logrus.Logger
}

type Fields map[string]interface{}

func NewLogger(level string) *Logger {
	logger := logrus.New()
	
	// JSON formatter for structured logging
	logger.SetFormatter(&logrus.JSONFormatter{
		TimestampFormat: time.RFC3339,
		FieldMap: logrus.FieldMap{
			logrus.FieldKeyTime:  "timestamp",
			logrus.FieldKeyLevel: "level",
			logrus.FieldKeyMsg:   "message",
		},
	})

	logger.SetOutput(os.Stdout)

	// Set log level
	switch level {
	case "debug":
		logger.SetLevel(logrus.DebugLevel)
	case "info":
		logger.SetLevel(logrus.InfoLevel)
	case "warn":
		logger.SetLevel(logrus.WarnLevel)
	case "error":
		logger.SetLevel(logrus.ErrorLevel)
	default:
		logger.SetLevel(logrus.InfoLevel)
	}

	return &Logger{Logger: logger}
}

func (l *Logger) WithContext(ctx context.Context) *logrus.Entry {
	entry := l.Logger.WithContext(ctx)
	
	// Add tracing information if available
	if span := opentracing.SpanFromContext(ctx); span != nil {
		if spanCtx, ok := span.Context().(jaeger.SpanContext); ok {
			entry = entry.WithFields(logrus.Fields{
				"trace_id": spanCtx.TraceID().String(),
				"span_id":  spanCtx.SpanID().String(),
			})
		}
	}

	return entry
}

func (l *Logger) WithFields(fields Fields) *logrus.Entry {
	return l.Logger.WithFields(logrus.Fields(fields))
}

func (l *Logger) WithError(err error) *logrus.Entry {
	return l.Logger.WithError(err)
}

// Business logic logging helpers
func (l *Logger) LogCustomerProcessing(ctx context.Context, customerID int64, operation string, duration time.Duration) {
	l.WithContext(ctx).WithFields(logrus.Fields{
		"customer_id": customerID,
		"operation":   operation,
		"duration_ms": duration.Milliseconds(),
		"component":   "pipeline",
	}).Info("Customer processing completed")
}

func (l *Logger) LogS3Upload(ctx context.Context, bucket, key string, size int64, duration time.Duration) {
	l.WithContext(ctx).WithFields(logrus.Fields{
		"s3_bucket":   bucket,
		"s3_key":      key,
		"size_bytes":  size,
		"duration_ms": duration.Milliseconds(),
		"component":   "s3",
	}).Info("S3 upload completed")
}

func (l *Logger) LogError(ctx context.Context, err error, component string, details Fields) {
	entry := l.WithContext(ctx).WithError(err).WithFields(logrus.Fields{
		"component": component,
	})
	
	if details != nil {
		entry = entry.WithFields(logrus.Fields(details))
	}
	
	entry.Error("Operation failed")
}

func (l *Logger) LogMetrics(ctx context.Context, metrics map[string]interface{}) {
	l.WithContext(ctx).WithFields(logrus.Fields{
		"component": "metrics",
		"metrics":   metrics,
	}).Info("Pipeline metrics")
}
```

### 4. Alerting Rules (`monitoring/alerts.yaml`)
```yaml
groups:
- name: big-machine-pipeline
  rules:
  - alert: PipelineDown
    expr: up{job="big-machine-pipeline"} == 0
    for: 1m
    labels:
      severity: critical
      component: pipeline
    annotations:
      summary: "Pipeline service is down"
      description: "Chinook pipeline has been down for more than 1 minute"
      runbook_url: "https://runbooks.chinook.com/pipeline-down"

  - alert: HighErrorRate
    expr: rate(pipeline_errors_total[5m]) > 0.1
    for: 2m
    labels:
      severity: warning
      component: pipeline
    annotations:
      summary: "High error rate detected"
      description: "Error rate is {{ $value }} errors per second"
      runbook_url: "https://runbooks.chinook.com/high-error-rate"

  - alert: ProcessingLag
    expr: time() - pipeline_last_success_timestamp > 300
    for: 1m
    labels:
      severity: warning
      component: pipeline
    annotations:
      summary: "Processing lag detected"
      description: "No successful processing in the last 5 minutes"

  - alert: S3UploadFailures
    expr: rate(pipeline_errors_total{type="s3_upload"}[5m]) > 0.01
    for: 1m
    labels:
      severity: warning
      component: s3
    annotations:
      summary: "S3 upload failures detected"
      description: "S3 upload error rate: {{ $value }} per second"

  - alert: DatabaseConnectionFailures
    expr: rate(pipeline_errors_total{type="database"}[5m]) > 0.01
    for: 30s
    labels:
      severity: critical
      component: database
    annotations:
      summary: "Database connection failures"
      description: "Database error rate: {{ $value }} per second"

  - alert: HighMemoryUsage
    expr: container_memory_usage_bytes{pod=~".*big-machine-pipeline.*"} / container_spec_memory_limit_bytes > 0.9
    for: 2m
    labels:
      severity: warning
      component: pipeline
    annotations:
      summary: "High memory usage"
      description: "Memory usage is above 90%"

  - alert: HighCPUUsage
    expr: rate(container_cpu_usage_seconds_total{pod=~".*big-machine-pipeline.*"}[5m]) > 0.8
    for: 5m
    labels:
      severity: warning
      component: pipeline
    annotations:
      summary: "High CPU usage"
      description: "CPU usage is above 80% for 5 minutes"
```

### 5. Grafana Dashboard (`monitoring/grafana-dashboard.json`)
```json
{
  "dashboard": {
    "id": null,
    "title": "Chinook Pipeline - Production Dashboard",
    "description": "Comprehensive monitoring for the Chinook data pipeline",
    "tags": ["pipeline", "production", "big-machine"],
    "timezone": "UTC",
    "refresh": "10s",
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "panels": [
      {
        "id": 1,
        "title": "Pipeline Status",
        "type": "stat",
        "gridPos": {"h": 4, "w": 6, "x": 0, "y": 0},
        "targets": [
          {
            "expr": "up{job=\"big-machine-pipeline\"}",
            "legendFormat": "Status"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "mappings": [
              {"options": {"0": {"text": "DOWN", "color": "red"}}},
              {"options": {"1": {"text": "UP", "color": "green"}}}
            ]
          }
        }
      },
      {
        "id": 2,
        "title": "Throughput",
        "type": "stat",
        "gridPos": {"h": 4, "w": 6, "x": 6, "y": 0},
        "targets": [
          {
            "expr": "rate(pipeline_customers_processed_total[5m])",
            "legendFormat": "Customers/sec"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "reqps",
            "decimals": 2
          }
        }
      },
      {
        "id": 3,
        "title": "Error Rate",
        "type": "stat",
        "gridPos": {"h": 4, "w": 6, "x": 12, "y": 0},
        "targets": [
          {
            "expr": "rate(pipeline_errors_total[5m])",
            "legendFormat": "Errors/sec"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "reqps",
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 0.01},
                {"color": "red", "value": 0.1}
              ]
            }
          }
        }
      },
      {
        "id": 4,
        "title": "Data Transfer",
        "type": "stat",
        "gridPos": {"h": 4, "w": 6, "x": 18, "y": 0},
        "targets": [
          {
            "expr": "rate(pipeline_bytes_uploaded_total[5m])",
            "legendFormat": "Bytes/sec"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "binBps"
          }
        }
      },
      {
        "id": 5,
        "title": "Processing Latency",
        "type": "graph",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 4},
        "targets": [
          {
            "expr": "histogram_quantile(0.50, rate(pipeline_processing_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          },
          {
            "expr": "histogram_quantile(0.95, rate(pipeline_processing_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.99, rate(pipeline_processing_duration_seconds_bucket[5m]))",
            "legendFormat": "99th percentile"
          }
        ],
        "yAxes": [
          {
            "unit": "s",
            "min": 0
          }
        ]
      },
      {
        "id": 6,
        "title": "Error Breakdown",
        "type": "piechart",
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 4},
        "targets": [
          {
            "expr": "sum by (type) (rate(pipeline_errors_total[5m]))",
            "legendFormat": "{{ type }}"
          }
        ]
      },
      {
        "id": 7,
        "title": "Resource Usage",
        "type": "graph",
        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 12},
        "targets": [
          {
            "expr": "rate(container_cpu_usage_seconds_total{pod=~\".*big-machine-pipeline.*\"}[5m]) * 100",
            "legendFormat": "CPU %"
          },
          {
            "expr": "container_memory_usage_bytes{pod=~\".*big-machine-pipeline.*\"} / 1024 / 1024",
            "legendFormat": "Memory MB"
          }
        ],
        "yAxes": [
          {
            "min": 0,
            "max": 100
          },
          {
            "min": 0,
            "unit": "decbytes"
          }
        ]
      }
    ]
  }
}
```

### 6. Observability Integration in Main Application

#### Updated Main with Full Observability (`cmd/s3-pipeline/main.go`)
```go
package main

import (
	"context"
	"database/sql"
	"encoding/json"
	"flag"
	"fmt"
	"log"
	"net/http"
	"os"
	"os/signal"
	"sync"
	"syscall"
	"time"

	_ "github.com/lib/pq"
	"google.golang.org/protobuf/proto"

	"big-machine-s3-pipeline/internal/health"
	"big-machine-s3-pipeline/internal/logging"
	"big-machine-s3-pipeline/internal/metrics"
	"big-machine-s3-pipeline/internal/s3"
	"big-machine-s3-pipeline/internal/tracing"
	pb "big-machine-s3-pipeline/proto"
)

type Config struct {
	DatabaseURL     string        `json:"DatabaseURL"`
	S3Bucket        string        `json:"S3Bucket"`
	S3Region        string        `json:"S3Region"`
	S3Prefix        string        `json:"S3Prefix"`
	BatchSize       int           `json:"BatchSize"`
	PollInterval    time.Duration `json:"PollInterval"`
	HealthPort      int           `json:"HealthPort"`
	LogLevel        string        `json:"LogLevel"`
	JaegerURL       string        `json:"JaegerURL"`
	JaegerSampleRate float64      `json:"JaegerSampleRate"`
}

type Pipeline struct {
	db                *sql.DB
	uploader          *s3.Uploader
	config            Config
	healthChecker     *health.Checker
	prometheusMetrics *metrics.PrometheusCollector
	logger            *logging.Logger
	shutdown          chan struct{}
	wg                sync.WaitGroup
}

func main() {
	var (
		configFile  = flag.String("config", "/config.json", "Configuration file path")
		healthCheck = flag.Bool("health-check", false, "Run health check and exit")
	)
	flag.Parse()

	if *healthCheck {
		if err := runHealthCheck(); err != nil {
			log.Printf("Health check failed: %v", err)
			os.Exit(1)
		}
		os.Exit(0)
	}

	config, err := loadConfig(*configFile)
	if err != nil {
		log.Fatalf("Failed to load config: %v", err)
	}

	// Initialize logger
	logger := logging.NewLogger(config.LogLevel)

	// Initialize tracing
	tracer, closer, err := tracing.InitTracing(tracing.TracingConfig{
		ServiceName: "big-machine-pipeline",
		JaegerURL:   config.JaegerURL,
		SampleRate:  config.JaegerSampleRate,
	})
	if err != nil {
		logger.WithError(err).Fatal("Failed to initialize tracing")
	}
	defer closer.Close()

	pipeline, err := NewPipeline(config, logger)
	if err != nil {
		logger.WithError(err).Fatal("Failed to create pipeline")
	}
	defer pipeline.Close()

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// Start tracing span for application lifecycle
	span, ctx := tracing.StartSpan(ctx, "pipeline.application.start")
	defer span.Finish()

	// Handle shutdown signals
	go pipeline.handleShutdown(cancel)

	logger.WithContext(ctx).Info("Starting Chinook S3 Pipeline with full observability")
	if err := pipeline.Run(ctx); err != nil {
		logger.WithContext(ctx).WithError(err).Fatal("Pipeline failed")
	}
	logger.WithContext(ctx).Info("Pipeline shutdown complete")
}

func NewPipeline(config Config, logger *logging.Logger) (*Pipeline, error) {
	// Connect to database with connection pooling
	db, err := sql.Open("postgres", config.DatabaseURL)
	if err != nil {
		return nil, fmt.Errorf("failed to connect to database: %w", err)
	}

	// Configure connection pool
	db.SetMaxOpenConns(25)
	db.SetMaxIdleConns(5)
	db.SetConnMaxLifetime(5 * time.Minute)

	if err := db.Ping(); err != nil {
		return nil, fmt.Errorf("failed to ping database: %w", err)
	}

	// Initialize S3 client
	s3Config := s3.Config{
		BucketName: config.S3Bucket,
		Region:     config.S3Region,
		Prefix:     config.S3Prefix,
	}

	s3Client, err := s3.NewS3Client(s3Config)
	if err != nil {
		return nil, fmt.Errorf("failed to create S3 client: %w", err)
	}

	// Health check S3
	if err := s3Client.HealthCheck(context.Background()); err != nil {
		return nil, fmt.Errorf("S3 health check failed: %w", err)
	}

	// Create uploader
	partitioner := s3.NewBusinessPartitioner(config.S3Prefix)
	uploaderConfig := s3.UploaderConfig{
		MaxRetries:  3,
		RetryDelay:  time.Second * 2,
		PartSize:    64 * 1024 * 1024,
		Concurrency: 5,
	}
	uploader := s3.NewUploader(s3Client, partitioner, uploaderConfig)

	// Initialize health checker
	healthChecker := health.NewChecker()
	healthChecker.AddCheck("database", func() error {
		return db.Ping()
	})
	healthChecker.AddCheck("s3", func() error {
		return s3Client.HealthCheck(context.Background())
	})

	// Initialize Prometheus metrics
	prometheusMetrics := metrics.NewPrometheusCollector()

	return &Pipeline{
		db:                db,
		uploader:          uploader,
		config:            config,
		healthChecker:     healthChecker,
		prometheusMetrics: prometheusMetrics,
		logger:            logger,
		shutdown:          make(chan struct{}),
	}, nil
}

func (p *Pipeline) processCustomers(ctx context.Context, lastID int) (int, int64, error) {
	// Create processing span
	span, ctx := tracing.StartSpanWithTags(ctx, "pipeline.process_customers", map[string]interface{}{
		"batch.last_id": lastID,
		"batch.size":    p.config.BatchSize,
	})
	defer span.Finish()

	// Start timing
	start := time.Now()
	defer func() {
		duration := time.Since(start)
		p.prometheusMetrics.RecordProcessingDuration(duration, "process_customers", fmt.Sprintf("%d", p.config.BatchSize))
	}()

	// Increment active batches
	p.prometheusMetrics.SetActiveBatches(1)
	defer p.prometheusMetrics.SetActiveBatches(0)

	query := `
		SELECT id, name, email, phone, status, created_at, updated_at 
		FROM customers 
		WHERE id > $1 
		ORDER BY id 
		LIMIT $2
	`

	// Trace database query
	dbSpan, ctx := tracing.TraceDBQuery(ctx, query, lastID, p.config.BatchSize)
	rows, err := p.db.QueryContext(ctx, query, lastID, p.config.BatchSize)
	if err != nil {
		tracing.RecordError(dbSpan, err)
		dbSpan.Finish()
		p.prometheusMetrics.RecordError("database", "error", "query")
		p.logger.LogError(ctx, err, "database", logging.Fields{
			"query":   query,
			"last_id": lastID,
			"limit":   p.config.BatchSize,
		})
		return 0, 0, fmt.Errorf("failed to query customers: %w", err)
	}
	dbSpan.Finish()
	defer rows.Close()

	var uploadItems []s3.UploadItem
	processed := 0
	totalBytes := int64(0)

	for rows.Next() {
		var customer pb.Customer
		var createdAt, updatedAt time.Time

		err := rows.Scan(
			&customer.Id,
			&customer.Name,
			&customer.Email,
			&customer.Phone,
			&customer.Status,
			&createdAt,
			&updatedAt,
		)
		if err != nil {
			p.logger.LogError(ctx, err, "database", logging.Fields{
				"operation": "scan_row",
			})
			continue
		}

		customer.CreatedAt = createdAt.Unix()
		customer.UpdatedAt = updatedAt.Unix()

		// Serialize protobuf
		serializeSpan, _ := tracing.StartSpan(ctx, "protobuf.marshal")
		data, err := proto.Marshal(&customer)
		if err != nil {
			tracing.RecordError(serializeSpan, err)
			serializeSpan.Finish()
			p.prometheusMetrics.RecordError("protobuf", "error", "marshal")
			p.logger.LogError(ctx, err, "protobuf", logging.Fields{
				"customer_id": customer.Id,
				"operation":   "marshal",
			})
			continue
		}
		serializeSpan.Finish()

		uploadItems = append(uploadItems, s3.UploadItem{
			EntityType: "customer",
			EntityID:   fmt.Sprintf("%d", customer.Id),
			Data:       data,
			Metadata: map[string]string{
				"customer-status": customer.Status,
				"record-version":  "1.0",
			},
		})

		totalBytes += int64(len(data))
		processed++

		// Log individual customer processing
		p.logger.LogCustomerProcessing(ctx, customer.Id, "processed", time.Since(start))
	}

	if err := rows.Err(); err != nil {
		tracing.RecordError(span, err)
		p.prometheusMetrics.RecordError("database", "error", "iteration")
		return processed, totalBytes, fmt.Errorf("error iterating customer rows: %w", err)
	}

	// Upload batch to S3 if we have items
	if len(uploadItems) > 0 {
		uploadStart := time.Now()
		uploadSpan, _ := tracing.TraceS3Upload(ctx, p.config.S3Bucket, "batch", totalBytes)
		
		_, err := p.uploader.UploadBatch(ctx, uploadItems)
		uploadDuration := time.Since(uploadStart)
		
		if err != nil {
			tracing.RecordError(uploadSpan, err)
			uploadSpan.Finish()
			p.prometheusMetrics.RecordError("s3", "error", "upload")
			p.logger.LogError(ctx, err, "s3", logging.Fields{
				"batch_size":    len(uploadItems),
				"total_bytes":   totalBytes,
				"upload_duration": uploadDuration,
			})
			return processed, totalBytes, fmt.Errorf("failed to upload batch: %w", err)
		}
		
		uploadSpan.Finish()
		
		// Record successful metrics
		p.prometheusMetrics.RecordUploadDuration(uploadDuration, p.config.S3Bucket, p.config.S3Region)
		p.prometheusMetrics.RecordBytesUploaded(totalBytes, "customer", "none")
		p.prometheusMetrics.RecordCustomersProcessed(processed, fmt.Sprintf("%d", p.config.BatchSize), "success")
		p.prometheusMetrics.RecordSuccess()
		
		p.logger.LogS3Upload(ctx, p.config.S3Bucket, "batch", totalBytes, uploadDuration)
	}

	// Log batch metrics
	p.logger.LogMetrics(ctx, map[string]interface{}{
		"customers_processed": processed,
		"bytes_uploaded":      totalBytes,
		"processing_duration": time.Since(start),
	})

	return processed, totalBytes, nil
}

// ... rest of the implementation similar to previous labs but with full observability integration
```

## ðŸ§ª Testing & Verification

### 1. Deploy Observability Stack
```bash
# Deploy Prometheus
kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/main/bundle.yaml

# Deploy Jaeger
kubectl create namespace observability
kubectl apply -n observability -f https://github.com/jaegertracing/jaeger-operator/releases/download/v1.29.1/jaeger-operator.yaml

# Deploy Grafana
helm repo add grafana https://grafana.github.io/helm-charts
helm install grafana grafana/grafana --namespace observability
```

### 2. Test Metrics Collection
```bash
# Port forward to Prometheus
kubectl port-forward svc/prometheus-operated 9090:9090

# Check metrics are being collected
curl http://localhost:9090/api/v1/query?query=pipeline_customers_processed_total

# Port forward to Grafana
kubectl port-forward svc/grafana 3000:80

# Access Grafana at http://localhost:3000
```

### 3. Test Distributed Tracing
```bash
# Port forward to Jaeger
kubectl port-forward svc/jaeger-query 16686:16686

# Access Jaeger UI at http://localhost:16686
# Look for "big-machine-pipeline" service traces
```

### 4. Test Alerting
```bash
# Create test load to trigger alerts
kubectl run load-test --image=busybox --rm -it -- /bin/sh
# while true; do wget -q -O- http://pipeline-service:8080/health; sleep 0.1; done

# Check if alerts fire in Prometheus
curl http://localhost:9090/api/v1/alerts
```

## âœ… Victory Conditions

- [ ] Prometheus metrics are collected and displayed
- [ ] Jaeger traces show complete request flows
- [ ] Structured logs are aggregated and searchable
- [ ] Alerts fire correctly for error conditions
- [ ] Grafana dashboards show real-time data
- [ ] Performance bottlenecks are visible in traces
- [ ] Error tracking provides actionable insights

## ðŸ”„ Advanced Features

### 1. Custom Metrics
```go
// Add business-specific metrics
pipelineRevenue := prometheus.NewGaugeVec(
    prometheus.GaugeOpts{
        Name: "pipeline_revenue_processed",
        Help: "Revenue value of processed customers",
    },
    []string{"customer_tier"},
)

// Track SLA compliance
slaCompliance := prometheus.NewHistogramVec(
    prometheus.HistogramOpts{
        Name: "pipeline_sla_compliance_seconds",
        Help: "Time to process customer data vs SLA",
        Buckets: []float64{30, 60, 300, 600, 1800}, // SLA buckets
    },
    []string{"sla_tier"},
)
```

### 2. Log Sampling
```go
// High-volume log sampling
if shouldSample := rand.Float64() < 0.1; shouldSample {
    logger.WithContext(ctx).Debug("Detailed processing info", 
        "customer_data", customer)
}
```

---

**ðŸŽ‰ Day 6 Complete!** Tomorrow: Final testing and production deployment! ðŸš€
