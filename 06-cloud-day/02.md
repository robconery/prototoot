# Lab 2: üê≥ Containerization & Optimization

Package your S3 pipeline into production-ready containers! Master Docker best practices, multi-stage builds, security scanning, and container optimization.

## üéØ Objectives
- Create optimized Docker images with multi-stage builds
- Implement health checks and graceful shutdown
- Build security-hardened containers
- Configure container monitoring and logging

## üê≥ Docker Implementation

### 1. Multi-Stage Dockerfile (`Dockerfile`)
```dockerfile
# Build stage
FROM golang:1.21-alpine AS builder

# Install dependencies for building
RUN apk add --no-cache git ca-certificates

# Set working directory
WORKDIR /app

# Copy go mod files first (better caching)
COPY go.mod go.sum ./
RUN go mod download

# Copy source code
COPY . .

# Build binary with optimizations
RUN CGO_ENABLED=0 GOOS=linux go build \
    -a -installsuffix cgo \
    -ldflags '-w -s -extldflags "-static"' \
    -o bin/s3-pipeline \
    cmd/s3-pipeline/main.go

# Runtime stage
FROM scratch

# Add CA certificates for HTTPS
COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/

# Add timezone data
COPY --from=builder /usr/share/zoneinfo /usr/share/zoneinfo

# Create non-root user
COPY --from=builder /etc/passwd /etc/passwd

# Copy binary
COPY --from=builder /app/bin/s3-pipeline /s3-pipeline

# Copy config template
COPY --from=builder /app/config.json /config.json

# Use non-root user
USER nobody

# Expose port for health checks
EXPOSE 8080

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD ["/s3-pipeline", "-health-check"]

# Run the binary
ENTRYPOINT ["/s3-pipeline"]
CMD ["-config=/config.json"]
```

### 2. Enhanced Main with Health Endpoint (`cmd/s3-pipeline/main.go`)
```go
package main

import (
	"context"
	"database/sql"
	"encoding/json"
	"flag"
	"fmt"
	"log"
	"net/http"
	"os"
	"os/signal"
	"sync"
	"syscall"
	"time"

	_ "github.com/lib/pq"
	"google.golang.org/protobuf/proto"

	"big-machine-s3-pipeline/internal/health"
	"big-machine-s3-pipeline/internal/metrics"
	"big-machine-s3-pipeline/internal/s3"
	pb "big-machine-s3-pipeline/proto"
)

type Config struct {
	DatabaseURL  string        `json:"DatabaseURL"`
	S3Bucket     string        `json:"S3Bucket"`
	S3Region     string        `json:"S3Region"`
	S3Prefix     string        `json:"S3Prefix"`
	BatchSize    int           `json:"BatchSize"`
	PollInterval time.Duration `json:"PollInterval"`
	HealthPort   int           `json:"HealthPort"`
	LogLevel     string        `json:"LogLevel"`
}

type Pipeline struct {
	db            *sql.DB
	uploader      *s3.Uploader
	config        Config
	healthChecker *health.Checker
	metrics       *metrics.Collector
	shutdown      chan struct{}
	wg            sync.WaitGroup
}

func main() {
	var (
		configFile  = flag.String("config", "/config.json", "Configuration file path")
		healthCheck = flag.Bool("health-check", false, "Run health check and exit")
	)
	flag.Parse()

	if *healthCheck {
		if err := runHealthCheck(); err != nil {
			log.Printf("Health check failed: %v", err)
			os.Exit(1)
		}
		os.Exit(0)
	}

	config, err := loadConfig(*configFile)
	if err != nil {
		log.Fatalf("Failed to load config: %v", err)
	}

	pipeline, err := NewPipeline(config)
	if err != nil {
		log.Fatalf("Failed to create pipeline: %v", err)
	}
	defer pipeline.Close()

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// Handle shutdown signals
	go pipeline.handleShutdown(cancel)

	log.Println("Starting Chinook S3 Pipeline...")
	if err := pipeline.Run(ctx); err != nil {
		log.Fatalf("Pipeline failed: %v", err)
	}
	log.Println("Pipeline shutdown complete")
}

func NewPipeline(config Config) (*Pipeline, error) {
	// Connect to database with connection pooling
	db, err := sql.Open("postgres", config.DatabaseURL)
	if err != nil {
		return nil, fmt.Errorf("failed to connect to database: %w", err)
	}

	// Configure connection pool
	db.SetMaxOpenConns(25)
	db.SetMaxIdleConns(5)
	db.SetConnMaxLifetime(5 * time.Minute)

	if err := db.Ping(); err != nil {
		return nil, fmt.Errorf("failed to ping database: %w", err)
	}

	// Initialize S3 client
	s3Config := s3.Config{
		BucketName: config.S3Bucket,
		Region:     config.S3Region,
		Prefix:     config.S3Prefix,
	}

	s3Client, err := s3.NewS3Client(s3Config)
	if err != nil {
		return nil, fmt.Errorf("failed to create S3 client: %w", err)
	}

	// Health check S3
	if err := s3Client.HealthCheck(context.Background()); err != nil {
		return nil, fmt.Errorf("S3 health check failed: %w", err)
	}

	// Create uploader
	partitioner := s3.NewBusinessPartitioner(config.S3Prefix)
	uploaderConfig := s3.UploaderConfig{
		MaxRetries:  3,
		RetryDelay:  time.Second * 2,
		PartSize:    64 * 1024 * 1024,
		Concurrency: 5,
	}
	uploader := s3.NewUploader(s3Client, partitioner, uploaderConfig)

	// Initialize health checker
	healthChecker := health.NewChecker()
	healthChecker.AddCheck("database", func() error {
		return db.Ping()
	})
	healthChecker.AddCheck("s3", func() error {
		return s3Client.HealthCheck(context.Background())
	})

	// Initialize metrics
	metricsCollector := metrics.NewCollector()

	return &Pipeline{
		db:            db,
		uploader:      uploader,
		config:        config,
		healthChecker: healthChecker,
		metrics:       metricsCollector,
		shutdown:      make(chan struct{}),
	}, nil
}

func (p *Pipeline) Run(ctx context.Context) error {
	// Start health server
	p.wg.Add(1)
	go p.startHealthServer(ctx)

	// Start metrics server
	p.wg.Add(1)
	go p.startMetricsServer(ctx)

	// Start main pipeline
	p.wg.Add(1)
	go p.runPipeline(ctx)

	// Wait for shutdown
	<-p.shutdown
	p.wg.Wait()

	return nil
}

func (p *Pipeline) runPipeline(ctx context.Context) {
	defer p.wg.Done()

	ticker := time.NewTicker(p.config.PollInterval)
	defer ticker.Stop()

	lastProcessedID := 0

	for {
		select {
		case <-ctx.Done():
			log.Println("Pipeline context cancelled")
			return
		case <-ticker.C:
			start := time.Now()
			processed, bytes, err := p.processCustomers(ctx, lastProcessedID)
			duration := time.Since(start)

			if err != nil {
				log.Printf("Error processing customers: %v", err)
				p.metrics.RecordError("pipeline_process_error")
				continue
			}

			if processed > 0 {
				lastProcessedID += processed
				p.metrics.RecordProcessing(processed, bytes, duration)
				log.Printf("Processed %d customers, %d bytes in %v", processed, bytes, duration)
			}
		}
	}
}

func (p *Pipeline) startHealthServer(ctx context.Context) {
	defer p.wg.Done()

	mux := http.NewServeMux()
	mux.HandleFunc("/health", p.healthChecker.Handler)
	mux.HandleFunc("/ready", p.readinessHandler)

	server := &http.Server{
		Addr:    fmt.Sprintf(":%d", p.config.HealthPort),
		Handler: mux,
	}

	go func() {
		<-ctx.Done()
		shutdownCtx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
		defer cancel()
		server.Shutdown(shutdownCtx)
	}()

	log.Printf("Health server starting on port %d", p.config.HealthPort)
	if err := server.ListenAndServe(); err != http.ErrServerClosed {
		log.Printf("Health server error: %v", err)
	}
}

func (p *Pipeline) startMetricsServer(ctx context.Context) {
	defer p.wg.Done()

	mux := http.NewServeMux()
	mux.HandleFunc("/metrics", p.metrics.Handler)

	server := &http.Server{
		Addr:    fmt.Sprintf(":%d", p.config.HealthPort+1),
		Handler: mux,
	}

	go func() {
		<-ctx.Done()
		shutdownCtx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
		defer cancel()
		server.Shutdown(shutdownCtx)
	}()

	log.Printf("Metrics server starting on port %d", p.config.HealthPort+1)
	if err := server.ListenAndServe(); err != http.ErrServerClosed {
		log.Printf("Metrics server error: %v", err)
	}
}

func (p *Pipeline) readinessHandler(w http.ResponseWriter, r *http.Request) {
	// Check if pipeline is ready (has processed at least one batch)
	stats := p.metrics.GetStats()
	if processed, ok := stats["total_processed"].(int64); ok && processed > 0 {
		w.WriteHeader(http.StatusOK)
		w.Write([]byte("ready"))
	} else {
		w.WriteHeader(http.StatusServiceUnavailable)
		w.Write([]byte("not ready"))
	}
}

func (p *Pipeline) processCustomers(ctx context.Context, lastID int) (int, int64, error) {
	query := `
		SELECT id, name, email, phone, status, created_at, updated_at 
		FROM customers 
		WHERE id > $1 
		ORDER BY id 
		LIMIT $2
	`

	rows, err := p.db.QueryContext(ctx, query, lastID, p.config.BatchSize)
	if err != nil {
		return 0, 0, fmt.Errorf("failed to query customers: %w", err)
	}
	defer rows.Close()

	var uploadItems []s3.UploadItem
	processed := 0
	totalBytes := int64(0)

	for rows.Next() {
		var customer pb.Customer
		var createdAt, updatedAt time.Time

		err := rows.Scan(
			&customer.Id,
			&customer.Name,
			&customer.Email,
			&customer.Phone,
			&customer.Status,
			&createdAt,
			&updatedAt,
		)
		if err != nil {
			log.Printf("Failed to scan customer row: %v", err)
			continue
		}

		customer.CreatedAt = createdAt.Unix()
		customer.UpdatedAt = updatedAt.Unix()

		data, err := proto.Marshal(&customer)
		if err != nil {
			log.Printf("Failed to marshal customer %d: %v", customer.Id, err)
			continue
		}

		uploadItems = append(uploadItems, s3.UploadItem{
			EntityType: "customer",
			EntityID:   fmt.Sprintf("%d", customer.Id),
			Data:       data,
			Metadata: map[string]string{
				"customer-status": customer.Status,
				"record-version":  "1.0",
			},
		})

		totalBytes += int64(len(data))
		processed++
	}

	if err := rows.Err(); err != nil {
		return processed, totalBytes, fmt.Errorf("error iterating customer rows: %w", err)
	}

	if len(uploadItems) > 0 {
		_, err := p.uploader.UploadBatch(ctx, uploadItems)
		if err != nil {
			return processed, totalBytes, fmt.Errorf("failed to upload batch: %w", err)
		}
	}

	return processed, totalBytes, nil
}

func (p *Pipeline) handleShutdown(cancel context.CancelFunc) {
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
	
	sig := <-sigChan
	log.Printf("Received signal: %v", sig)
	
	// Cancel context to stop all operations
	cancel()
	
	// Give time for graceful shutdown
	time.Sleep(2 * time.Second)
	
	close(p.shutdown)
}

func (p *Pipeline) Close() error {
	return p.db.Close()
}

func loadConfig(filename string) (Config, error) {
	var config Config

	file, err := os.Open(filename)
	if err != nil {
		return config, err
	}
	defer file.Close()

	decoder := json.NewDecoder(file)
	err = decoder.Decode(&config)
	
	// Set defaults
	if config.HealthPort == 0 {
		config.HealthPort = 8080
	}
	if config.LogLevel == "" {
		config.LogLevel = "info"
	}

	return config, err
}

func runHealthCheck() error {
	resp, err := http.Get("http://localhost:8080/health")
	if err != nil {
		return err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("health check failed with status: %d", resp.StatusCode)
	}

	return nil
}
```

### 3. Health Checker (`internal/health/checker.go`)
```go
package health

import (
	"encoding/json"
	"net/http"
	"sync"
	"time"
)

type Check func() error

type Checker struct {
	checks map[string]Check
	mu     sync.RWMutex
}

type HealthStatus struct {
	Status    string            `json:"status"`
	Timestamp time.Time         `json:"timestamp"`
	Checks    map[string]string `json:"checks"`
}

func NewChecker() *Checker {
	return &Checker{
		checks: make(map[string]Check),
	}
}

func (c *Checker) AddCheck(name string, check Check) {
	c.mu.Lock()
	defer c.mu.Unlock()
	c.checks[name] = check
}

func (c *Checker) Handler(w http.ResponseWriter, r *http.Request) {
	c.mu.RLock()
	defer c.mu.RUnlock()

	status := HealthStatus{
		Status:    "healthy",
		Timestamp: time.Now(),
		Checks:    make(map[string]string),
	}

	for name, check := range c.checks {
		if err := check(); err != nil {
			status.Status = "unhealthy"
			status.Checks[name] = err.Error()
		} else {
			status.Checks[name] = "ok"
		}
	}

	w.Header().Set("Content-Type", "application/json")
	if status.Status == "unhealthy" {
		w.WriteHeader(http.StatusServiceUnavailable)
	}

	json.NewEncoder(w).Encode(status)
}
```

### 4. Metrics Collector (`internal/metrics/collector.go`)
```go
package metrics

import (
	"encoding/json"
	"net/http"
	"sync"
	"time"
)

type Collector struct {
	mu             sync.RWMutex
	totalProcessed int64
	totalBytes     int64
	totalErrors    int64
	avgDuration    time.Duration
	lastProcess    time.Time
	errors         map[string]int64
}

type MetricsData struct {
	TotalProcessed int64             `json:"total_processed"`
	TotalBytes     int64             `json:"total_bytes"`
	TotalErrors    int64             `json:"total_errors"`
	AvgDuration    string            `json:"avg_duration"`
	LastProcess    time.Time         `json:"last_process"`
	ErrorBreakdown map[string]int64  `json:"error_breakdown"`
}

func NewCollector() *Collector {
	return &Collector{
		errors: make(map[string]int64),
	}
}

func (c *Collector) RecordProcessing(processed int, bytes int64, duration time.Duration) {
	c.mu.Lock()
	defer c.mu.Unlock()

	c.totalProcessed += int64(processed)
	c.totalBytes += bytes
	c.avgDuration = (c.avgDuration + duration) / 2
	c.lastProcess = time.Now()
}

func (c *Collector) RecordError(errorType string) {
	c.mu.Lock()
	defer c.mu.Unlock()

	c.totalErrors++
	c.errors[errorType]++
}

func (c *Collector) GetStats() map[string]interface{} {
	c.mu.RLock()
	defer c.mu.RUnlock()

	return map[string]interface{}{
		"total_processed": c.totalProcessed,
		"total_bytes":     c.totalBytes,
		"total_errors":    c.totalErrors,
		"avg_duration":    c.avgDuration,
		"last_process":    c.lastProcess,
	}
}

func (c *Collector) Handler(w http.ResponseWriter, r *http.Request) {
	c.mu.RLock()
	defer c.mu.RUnlock()

	metrics := MetricsData{
		TotalProcessed: c.totalProcessed,
		TotalBytes:     c.totalBytes,
		TotalErrors:    c.totalErrors,
		AvgDuration:    c.avgDuration.String(),
		LastProcess:    c.lastProcess,
		ErrorBreakdown: c.errors,
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(metrics)
}
```

### 5. Docker Compose for Local Development (`docker-compose.yml`)
```yaml
version: '3.8'

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: chinook
      POSTGRES_USER: dev
      POSTGRES_PASSWORD: devpass123
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql

  pipeline:
    build: .
    depends_on:
      - postgres
    environment:
      - DATABASE_URL=postgres://dev:devpass123@postgres:5432/chinook?sslmode=disable
      - S3_BUCKET=${S3_BUCKET}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=us-west-2
    ports:
      - "8080:8080"
      - "8081:8081"
    volumes:
      - ./config.docker.json:/config.json:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "/s3-pipeline", "-health-check"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  localstack:
    image: localstack/localstack:latest
    environment:
      - SERVICES=s3
      - DEBUG=1
      - DATA_DIR=/tmp/localstack/data
    ports:
      - "4566:4566"
    volumes:
      - localstack_data:/tmp/localstack

volumes:
  postgres_data:
  localstack_data:
```

### 6. Security Scanning Script (`scripts/security-scan.sh`)
```bash
#!/bin/bash

set -e

IMAGE_NAME="big-machine-pipeline"
TAG="latest"

echo "üîç Running security scans for $IMAGE_NAME:$TAG"

# Build image if not exists
if ! docker image inspect "$IMAGE_NAME:$TAG" >/dev/null 2>&1; then
    echo "Building image $IMAGE_NAME:$TAG"
    docker build -t "$IMAGE_NAME:$TAG" .
fi

# Scan with Trivy (install if needed)
if ! command -v trivy &> /dev/null; then
    echo "Installing Trivy..."
    curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin
fi

echo "üõ°Ô∏è  Scanning for vulnerabilities..."
trivy image --severity HIGH,CRITICAL "$IMAGE_NAME:$TAG"

# Check image size
echo "üìè Image size analysis:"
docker images "$IMAGE_NAME:$TAG" --format "table {{.Repository}}\t{{.Tag}}\t{{.Size}}"

# Analyze layers
echo "üç∞ Layer analysis:"
docker history "$IMAGE_NAME:$TAG" --no-trunc

echo "‚úÖ Security scan complete!"
```

### 7. Build and Deploy Script (`scripts/build-deploy.sh`)
```bash
#!/bin/bash

set -e

VERSION=${1:-latest}
REGISTRY=${2:-your-registry.com}
IMAGE_NAME="big-machine-pipeline"

echo "üöÄ Building and deploying $IMAGE_NAME:$VERSION"

# Build multi-arch image
echo "üèóÔ∏è  Building multi-architecture image..."
docker buildx create --use --name multiarch || true
docker buildx build \
    --platform linux/amd64,linux/arm64 \
    --tag "$REGISTRY/$IMAGE_NAME:$VERSION" \
    --tag "$REGISTRY/$IMAGE_NAME:latest" \
    --push .

# Security scan
echo "üîç Running security scan..."
./scripts/security-scan.sh

# Generate SBOM (Software Bill of Materials)
echo "üìã Generating SBOM..."
docker buildx imagetools inspect "$REGISTRY/$IMAGE_NAME:$VERSION" --format '{{ json . }}' > sbom.json

echo "‚úÖ Build and deploy complete!"
echo "Image: $REGISTRY/$IMAGE_NAME:$VERSION"
```

## üß™ Testing & Verification

### 1. Build and Test Locally
```bash
# Build the image
docker build -t big-machine-pipeline:latest .

# Test with docker-compose
docker-compose up -d

# Check health
curl http://localhost:8080/health

# Check metrics
curl http://localhost:8081/metrics

# Check logs
docker-compose logs -f pipeline
```

### 2. Container Security Testing
```bash
# Run security scan
./scripts/security-scan.sh

# Test as non-root user
docker run --rm big-machine-pipeline:latest whoami

# Check file permissions
docker run --rm big-machine-pipeline:latest ls -la /
```

### 3. Performance Testing
```bash
# Monitor resource usage
docker stats

# Load testing with multiple containers
docker-compose up --scale pipeline=3

# Memory profiling
docker exec -it $(docker-compose ps -q pipeline) ps aux
```

## ‚úÖ Victory Conditions

- [ ] Multi-stage Docker build creates optimized image (<50MB)
- [ ] Container runs as non-root user
- [ ] Health checks and readiness probes work correctly
- [ ] Graceful shutdown handles SIGTERM properly
- [ ] Security scan shows no HIGH/CRITICAL vulnerabilities
- [ ] Container metrics are exposed and accurate
- [ ] Docker Compose stack runs successfully

## üîÑ Advanced Topics

### 1. Container Resource Limits
```yaml
# In docker-compose.yml
services:
  pipeline:
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
```

### 2. Log Aggregation
```go
// Add structured logging
import "github.com/sirupsen/logrus"

func setupLogging() {
    logrus.SetFormatter(&logrus.JSONFormatter{})
    logrus.SetLevel(logrus.InfoLevel)
}
```

---

**Next**: Head to `lab03/` for Kubernetes deployment! ‚ò∏Ô∏è
