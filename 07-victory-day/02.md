# Lab 2: ‚ö° Performance & Optimization

**Duration**: 50 minutes  
**Difficulty**: Advanced  

## üéØ Objectives
- Profile CPU and memory usage to identify bottlenecks
- Optimize database queries and connection pooling
- Implement caching strategies for improved performance
- Tune protobuf serialization for maximum throughput

## üìã Prerequisites
- Completed Lab 1 (testing infrastructure in place)
- Go profiling tools (pprof)
- Access to your running Chinook pipeline

## üõ†Ô∏è Tasks

### Task 1: CPU and Memory Profiling (15 minutes)

Let's identify performance bottlenecks using Go's built-in profiling tools.

**File: `cmd/profiler/main.go`**
```go
package main

import (
	"context"
	"fmt"
	"log"
	"net/http"
	_ "net/http/pprof"
	"os"
	"runtime"
	"runtime/pprof"
	"sync"
	"time"

	"chinook/internal/customer"
	pb "chinook/proto/customer/v1"
)

type PerformanceProfiler struct {
	customerSvc *customer.Service
	results     map[string]ProfileResult
	mu          sync.RWMutex
}

type ProfileResult struct {
	Operation     string
	Duration      time.Duration
	MemoryBefore  runtime.MemStats
	MemoryAfter   runtime.MemStats
	AllocsPerOp   int64
	BytesPerOp    int64
}

func main() {
	fmt.Println("‚ö° Chinook Performance Profiler")
	fmt.Println("===================================")

	// Start pprof server in background
	go func() {
		log.Println("üîç pprof server starting on :6060")
		log.Println("   CPU Profile: http://localhost:6060/debug/pprof/profile")
		log.Println("   Memory Profile: http://localhost:6060/debug/pprof/heap")
		log.Println("   Goroutines: http://localhost:6060/debug/pprof/goroutine")
		log.Fatal(http.ListenAndServe(":6060", nil))
	}()

	profiler := NewPerformanceProfiler()
	
	// Run performance analysis
	fmt.Println("\nüéØ Starting performance analysis...")
	profiler.RunAnalysis()
	
	// Generate optimization recommendations
	profiler.GenerateRecommendations()
}

func NewPerformanceProfiler() *PerformanceProfiler {
	// Initialize customer service with realistic configuration
	db := initializeDatabase()
	customerSvc := customer.NewService(db)

	return &PerformanceProfiler{
		customerSvc: customerSvc,
		results:     make(map[string]ProfileResult),
	}
}

func (pp *PerformanceProfiler) RunAnalysis() {
	operations := []struct {
		name string
		fn   func() error
	}{
		{"Create Customer", pp.profileCreateOperation},
		{"Batch Create", pp.profileBatchCreateOperation},
		{"Query Single", pp.profileQuerySingleOperation},
		{"Query Batch", pp.profileQueryBatchOperation},
		{"Protobuf Serialization", pp.profileSerializationOperation},
		{"Memory Allocation", pp.profileMemoryOperation},
	}

	for _, op := range operations {
		fmt.Printf("üìä Profiling: %s\n", op.name)
		result := pp.runSingleProfile(op.name, op.fn)
		
		pp.mu.Lock()
		pp.results[op.name] = result
		pp.mu.Unlock()

		pp.displayResult(result)
		fmt.Println("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
	}
}

func (pp *PerformanceProfiler) runSingleProfile(operation string, fn func() error) ProfileResult {
	// Force GC before measurement
	runtime.GC()
	runtime.GC()

	var memBefore, memAfter runtime.MemStats
	runtime.ReadMemStats(&memBefore)

	// CPU profiling
	cpuFile, err := os.Create(fmt.Sprintf("cpu_%s.prof", 
		sanitizeFilename(operation)))
	if err != nil {
		log.Printf("Failed to create CPU profile: %v", err)
	} else {
		defer cpuFile.Close()
		pprof.StartCPUProfile(cpuFile)
		defer pprof.StopCPUProfile()
	}

	start := time.Now()
	
	// Run the operation multiple times for accurate measurement
	iterations := 1000
	for i := 0; i < iterations; i++ {
		if err := fn(); err != nil {
			log.Printf("Operation failed on iteration %d: %v", i, err)
		}
	}

	duration := time.Since(start)
	runtime.ReadMemStats(&memAfter)

	// Memory profiling
	memFile, err := os.Create(fmt.Sprintf("mem_%s.prof", 
		sanitizeFilename(operation)))
	if err != nil {
		log.Printf("Failed to create memory profile: %v", err)
	} else {
		defer memFile.Close()
		pprof.WriteHeapProfile(memFile)
	}

	return ProfileResult{
		Operation:     operation,
		Duration:      duration,
		MemoryBefore:  memBefore,
		MemoryAfter:   memAfter,
		AllocsPerOp:   int64(memAfter.Mallocs-memBefore.Mallocs) / int64(iterations),
		BytesPerOp:    int64(memAfter.TotalAlloc-memBefore.TotalAlloc) / int64(iterations),
	}
}

func (pp *PerformanceProfiler) profileCreateOperation() error {
	ctx := context.Background()
	customer := &pb.Customer{
		Id:    fmt.Sprintf("perf-test-%d", time.Now().UnixNano()),
		Email: "perf@chinook.com",
		Name:  "Performance Test User",
		Phone: "+15551234567",
	}
	return pp.customerSvc.Create(ctx, customer)
}

func (pp *PerformanceProfiler) profileBatchCreateOperation() error {
	ctx := context.Background()
	customers := make([]*pb.Customer, 10)
	
	for i := range customers {
		customers[i] = &pb.Customer{
			Id:    fmt.Sprintf("batch-%d-%d", time.Now().UnixNano(), i),
			Email: fmt.Sprintf("batch%d@chinook.com", i),
			Name:  fmt.Sprintf("Batch User %d", i),
			Phone: fmt.Sprintf("+1555%07d", i),
		}
	}

	return pp.customerSvc.CreateBatch(ctx, customers)
}

func (pp *PerformanceProfiler) profileQuerySingleOperation() error {
	ctx := context.Background()
	_, err := pp.customerSvc.GetByID(ctx, "existing-customer-id")
	return err
}

func (pp *PerformanceProfiler) profileQueryBatchOperation() error {
	ctx := context.Background()
	_, err := pp.customerSvc.List(ctx, 100, 0)
	return err
}

func (pp *PerformanceProfiler) profileSerializationOperation() error {
	customer := &pb.Customer{
		Id:    "serialization-test",
		Email: "serialize@chinook.com",
		Name:  "Serialization Test User",
		Phone: "+15551234567",
		Metadata: map[string]string{
			"key1": "value1",
			"key2": "value2",
			"key3": "value3",
		},
	}

	// Marshal to protobuf
	data, err := proto.Marshal(customer)
	if err != nil {
		return err
	}

	// Unmarshal from protobuf
	var unmarshaled pb.Customer
	return proto.Unmarshal(data, &unmarshaled)
}

func (pp *PerformanceProfiler) profileMemoryOperation() error {
	// Allocate various sized objects to test memory patterns
	data := make([]byte, 1024*1024) // 1MB
	_ = data
	
	customers := make([]*pb.Customer, 1000)
	for i := range customers {
		customers[i] = &pb.Customer{
			Id:    fmt.Sprintf("mem-test-%d", i),
			Email: fmt.Sprintf("mem%d@chinook.com", i),
		}
	}
	_ = customers

	return nil
}

func (pp *PerformanceProfiler) displayResult(result ProfileResult) {
	fmt.Printf("   ‚è±Ô∏è  Duration: %v\n", result.Duration)
	fmt.Printf("   üß† Allocs/op: %d\n", result.AllocsPerOp)
	fmt.Printf("   üìè Bytes/op: %d\n", result.BytesPerOp)
	
	memDiff := result.MemoryAfter.Alloc - result.MemoryBefore.Alloc
	fmt.Printf("   üìä Memory Œî: %s\n", formatBytes(int64(memDiff)))
}

func (pp *PerformanceProfiler) GenerateRecommendations() {
	fmt.Printf("\nüéØ Performance Optimization Recommendations\n")
	fmt.Printf("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n")

	recommendations := []string{}

	// Analyze each operation
	for name, result := range pp.results {
		if result.AllocsPerOp > 10 {
			recommendations = append(recommendations, 
				fmt.Sprintf("üîç %s: High allocation count (%d/op) - consider object pooling", 
					name, result.AllocsPerOp))
		}

		if result.BytesPerOp > 1024 {
			recommendations = append(recommendations, 
				fmt.Sprintf("üì¶ %s: High memory usage (%s/op) - review data structures", 
					name, formatBytes(result.BytesPerOp)))
		}
	}

	// General recommendations
	recommendations = append(recommendations,
		"‚ö° Enable connection pooling with proper limits",
		"üóÉÔ∏è  Implement query result caching for frequently accessed data",
		"üì¶ Use protobuf field options to optimize serialization",
		"üîÑ Consider batch operations for bulk data processing",
		"üìä Monitor GC pressure and tune GOGC if needed",
	)

	for i, rec := range recommendations {
		fmt.Printf("%d. %s\n", i+1, rec)
	}

	fmt.Printf("\nüìà Next Steps:\n")
	fmt.Printf("   ‚Ä¢ Analyze pprof profiles: go tool pprof cpu_*.prof\n")
	fmt.Printf("   ‚Ä¢ Review memory profiles: go tool pprof mem_*.prof\n")
	fmt.Printf("   ‚Ä¢ Implement caching layer (next task)\n")
	fmt.Printf("   ‚Ä¢ Optimize database queries\n")
}

func sanitizeFilename(s string) string {
	// Replace spaces and special chars for filename
	return strings.ReplaceAll(strings.ToLower(s), " ", "_")
}

func formatBytes(bytes int64) string {
	const unit = 1024
	if bytes < unit {
		return fmt.Sprintf("%d B", bytes)
	}
	div, exp := int64(unit), 0
	for n := bytes / unit; n >= unit; n /= unit {
		div *= unit
		exp++
	}
	return fmt.Sprintf("%.1f %cB", float64(bytes)/float64(div), "KMGTPE"[exp])
}

func initializeDatabase() *sql.DB {
	// Database initialization - implement based on your setup
	// This is a placeholder
	return nil
}
```

**Run the profiler:**
```bash
go run cmd/profiler/main.go
```

**Analyze profiles:**
```bash
# CPU profile analysis
go tool pprof cpu_create_customer.prof
# (pprof) top10
# (pprof) web

# Memory profile analysis  
go tool pprof mem_create_customer.prof
# (pprof) top10
# (pprof) list main.profileCreateOperation
```

### Task 2: Database Optimization (15 minutes)

Optimize your database queries and connection handling.

**File: `internal/customer/repository_optimized.go`**
```go
package customer

import (
	"context"
	"database/sql"
	"fmt"
	"strings"
	"sync"
	"time"

	"github.com/lib/pq"
	pb "chinook/proto/customer/v1"
)

type OptimizedRepository struct {
	db          *sql.DB
	stmtCache   map[string]*sql.Stmt
	cacheMutex  sync.RWMutex
	queryCache  *QueryCache
}

type QueryCache struct {
	cache map[string]CacheEntry
	mutex sync.RWMutex
	ttl   time.Duration
}

type CacheEntry struct {
	data      interface{}
	timestamp time.Time
}

func NewOptimizedRepository(db *sql.DB) *OptimizedRepository {
	// Configure database for optimal performance
	db.SetMaxOpenConns(25)                 // Limit connections
	db.SetMaxIdleConns(5)                  // Keep some idle
	db.SetConnMaxLifetime(30 * time.Minute) // Rotate connections

	return &OptimizedRepository{
		db:         db,
		stmtCache:  make(map[string]*sql.Stmt),
		queryCache: NewQueryCache(5 * time.Minute),
	}
}

func NewQueryCache(ttl time.Duration) *QueryCache {
	cache := &QueryCache{
		cache: make(map[string]CacheEntry),
		ttl:   ttl,
	}

	// Start cleanup goroutine
	go cache.cleanup()
	
	return cache
}

func (qc *QueryCache) Get(key string) (interface{}, bool) {
	qc.mutex.RLock()
	defer qc.mutex.RUnlock()

	entry, exists := qc.cache[key]
	if !exists {
		return nil, false
	}

	// Check if expired
	if time.Since(entry.timestamp) > qc.ttl {
		go qc.delete(key) // Async cleanup
		return nil, false
	}

	return entry.data, true
}

func (qc *QueryCache) Set(key string, data interface{}) {
	qc.mutex.Lock()
	defer qc.mutex.Unlock()

	qc.cache[key] = CacheEntry{
		data:      data,
		timestamp: time.Now(),
	}
}

func (qc *QueryCache) delete(key string) {
	qc.mutex.Lock()
	defer qc.mutex.Unlock()
	delete(qc.cache, key)
}

func (qc *QueryCache) cleanup() {
	ticker := time.NewTicker(time.Minute)
	defer ticker.Stop()

	for range ticker.C {
		qc.mutex.Lock()
		now := time.Now()
		for key, entry := range qc.cache {
			if now.Sub(entry.timestamp) > qc.ttl {
				delete(qc.cache, key)
			}
		}
		qc.mutex.Unlock()
	}
}

// Optimized single customer creation with prepared statement
func (r *OptimizedRepository) CreateOptimized(ctx context.Context, customer *pb.Customer) error {
	stmt, err := r.getOrPrepareStmt("create_customer", `
		INSERT INTO customers (id, email, name, phone, metadata, created_at, updated_at)
		VALUES ($1, $2, $3, $4, $5, $6, $7)
	`)
	if err != nil {
		return fmt.Errorf("failed to prepare statement: %w", err)
	}

	now := time.Now()
	_, err = stmt.ExecContext(ctx,
		customer.Id,
		customer.Email,
		customer.Name,
		customer.Phone,
		customer.Metadata,
		now,
		now,
	)

	if err != nil {
		return fmt.Errorf("failed to create customer: %w", err)
	}

	// Invalidate relevant cache entries
	r.queryCache.delete("list_customers")
	r.queryCache.delete(fmt.Sprintf("customer_%s", customer.Id))

	return nil
}

// Optimized batch creation using COPY
func (r *OptimizedRepository) CreateBatchOptimized(ctx context.Context, customers []*pb.Customer) error {
	if len(customers) == 0 {
		return nil
	}

	// Use PostgreSQL COPY for maximum performance
	txn, err := r.db.BeginTx(ctx, nil)
	if err != nil {
		return fmt.Errorf("failed to begin transaction: %w", err)
	}
	defer txn.Rollback()

	stmt, err := txn.PrepareContext(ctx, pq.CopyIn("customers",
		"id", "email", "name", "phone", "metadata", "created_at", "updated_at"))
	if err != nil {
		return fmt.Errorf("failed to prepare copy statement: %w", err)
	}

	now := time.Now()
	for _, customer := range customers {
		_, err = stmt.ExecContext(ctx,
			customer.Id,
			customer.Email,
			customer.Name,
			customer.Phone,
			customer.Metadata,
			now,
			now,
		)
		if err != nil {
			return fmt.Errorf("failed to add customer to batch: %w", err)
		}
	}

	_, err = stmt.ExecContext(ctx)
	if err != nil {
		return fmt.Errorf("failed to execute batch: %w", err)
	}

	err = stmt.Close()
	if err != nil {
		return fmt.Errorf("failed to close copy statement: %w", err)
	}

	err = txn.Commit()
	if err != nil {
		return fmt.Errorf("failed to commit transaction: %w", err)
	}

	// Clear cache
	r.queryCache.delete("list_customers")

	return nil
}

// Optimized single query with caching
func (r *OptimizedRepository) GetByIDOptimized(ctx context.Context, id string) (*pb.Customer, error) {
	cacheKey := fmt.Sprintf("customer_%s", id)
	
	// Check cache first
	if cached, found := r.queryCache.Get(cacheKey); found {
		return cached.(*pb.Customer), nil
	}

	stmt, err := r.getOrPrepareStmt("get_customer", `
		SELECT id, email, name, phone, metadata, created_at, updated_at
		FROM customers 
		WHERE id = $1
	`)
	if err != nil {
		return nil, fmt.Errorf("failed to prepare statement: %w", err)
	}

	var customer pb.Customer
	var createdAt, updatedAt time.Time
	
	err = stmt.QueryRowContext(ctx, id).Scan(
		&customer.Id,
		&customer.Email,
		&customer.Name,
		&customer.Phone,
		&customer.Metadata,
		&createdAt,
		&updatedAt,
	)

	if err != nil {
		if err == sql.ErrNoRows {
			return nil, ErrCustomerNotFound
		}
		return nil, fmt.Errorf("failed to query customer: %w", err)
	}

	// Cache the result
	r.queryCache.Set(cacheKey, &customer)

	return &customer, nil
}

// Optimized list query with pagination and caching
func (r *OptimizedRepository) ListOptimized(ctx context.Context, limit, offset int) ([]*pb.Customer, error) {
	cacheKey := fmt.Sprintf("list_customers_%d_%d", limit, offset)
	
	// Check cache
	if cached, found := r.queryCache.Get(cacheKey); found {
		return cached.([]*pb.Customer), nil
	}

	stmt, err := r.getOrPrepareStmt("list_customers", `
		SELECT id, email, name, phone, metadata, created_at, updated_at
		FROM customers 
		ORDER BY created_at DESC
		LIMIT $1 OFFSET $2
	`)
	if err != nil {
		return nil, fmt.Errorf("failed to prepare statement: %w", err)
	}

	rows, err := stmt.QueryContext(ctx, limit, offset)
	if err != nil {
		return nil, fmt.Errorf("failed to execute query: %w", err)
	}
	defer rows.Close()

	var customers []*pb.Customer
	for rows.Next() {
		var customer pb.Customer
		var createdAt, updatedAt time.Time

		err := rows.Scan(
			&customer.Id,
			&customer.Email,
			&customer.Name,
			&customer.Phone,
			&customer.Metadata,
			&createdAt,
			&updatedAt,
		)
		if err != nil {
			return nil, fmt.Errorf("failed to scan customer: %w", err)
		}

		customers = append(customers, &customer)
	}

	if err = rows.Err(); err != nil {
		return nil, fmt.Errorf("row iteration error: %w", err)
	}

	// Cache the results
	r.queryCache.Set(cacheKey, customers)

	return customers, nil
}

// Statement caching to avoid repreparing queries
func (r *OptimizedRepository) getOrPrepareStmt(key, query string) (*sql.Stmt, error) {
	r.cacheMutex.RLock()
	if stmt, exists := r.stmtCache[key]; exists {
		r.cacheMutex.RUnlock()
		return stmt, nil
	}
	r.cacheMutex.RUnlock()

	// Prepare new statement
	stmt, err := r.db.Prepare(query)
	if err != nil {
		return nil, err
	}

	r.cacheMutex.Lock()
	r.stmtCache[key] = stmt
	r.cacheMutex.Unlock()

	return stmt, nil
}

// Close cleans up prepared statements
func (r *OptimizedRepository) Close() error {
	r.cacheMutex.Lock()
	defer r.cacheMutex.Unlock()

	for _, stmt := range r.stmtCache {
		stmt.Close()
	}

	return nil
}
```

### Task 3: Protobuf Serialization Optimization (10 minutes)

Optimize protobuf serialization for better performance.

**File: `internal/serialization/optimizer.go`**
```go
package serialization

import (
	"bytes"
	"compress/gzip"
	"fmt"
	"io"
	"sync"

	"google.golang.org/protobuf/proto"
	"google.golang.org/protobuf/protoadapt"
	pb "chinook/proto/customer/v1"
)

type SerializationOptimizer struct {
	bufferPool   sync.Pool
	compressorPool sync.Pool
}

func NewSerializationOptimizer() *SerializationOptimizer {
	return &SerializationOptimizer{
		bufferPool: sync.Pool{
			New: func() interface{} {
				return bytes.NewBuffer(make([]byte, 0, 4096)) // 4KB initial capacity
			},
		},
		compressorPool: sync.Pool{
			New: func() interface{} {
				return gzip.NewWriter(nil)
			},
		},
	}
}

// Optimized single message serialization
func (so *SerializationOptimizer) MarshalOptimized(customer *pb.Customer) ([]byte, error) {
	// Use buffer pool to avoid allocations
	buf := so.bufferPool.Get().(*bytes.Buffer)
	defer func() {
		buf.Reset()
		so.bufferPool.Put(buf)
	}()

	// Marshal to buffer
	data, err := proto.Marshal(customer)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal customer: %w", err)
	}

	return data, nil
}

// Optimized batch serialization with compression
func (so *SerializationOptimizer) MarshalBatchOptimized(customers []*pb.Customer, compress bool) ([]byte, error) {
	// Create a batch container
	batch := &pb.CustomerBatch{
		Customers: customers,
	}

	data, err := proto.Marshal(batch)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal batch: %w", err)
	}

	if !compress {
		return data, nil
	}

	// Compress if requested
	return so.compressData(data)
}

func (so *SerializationOptimizer) compressData(data []byte) ([]byte, error) {
	buf := so.bufferPool.Get().(*bytes.Buffer)
	defer func() {
		buf.Reset()
		so.bufferPool.Put(buf)
	}()

	compressor := so.compressorPool.Get().(*gzip.Writer)
	defer so.compressorPool.Put(compressor)

	compressor.Reset(buf)
	
	if _, err := compressor.Write(data); err != nil {
		return nil, fmt.Errorf("failed to compress data: %w", err)
	}

	if err := compressor.Close(); err != nil {
		return nil, fmt.Errorf("failed to close compressor: %w", err)
	}

	return buf.Bytes(), nil
}

// Optimized deserialization with decompression
func (so *SerializationOptimizer) UnmarshalBatchOptimized(data []byte, compressed bool) ([]*pb.Customer, error) {
	var err error
	
	if compressed {
		data, err = so.decompressData(data)
		if err != nil {
			return nil, err
		}
	}

	var batch pb.CustomerBatch
	if err := proto.Unmarshal(data, &batch); err != nil {
		return nil, fmt.Errorf("failed to unmarshal batch: %w", err)
	}

	return batch.Customers, nil
}

func (so *SerializationOptimizer) decompressData(data []byte) ([]byte, error) {
	reader, err := gzip.NewReader(bytes.NewReader(data))
	if err != nil {
		return nil, fmt.Errorf("failed to create gzip reader: %w", err)
	}
	defer reader.Close()

	buf := so.bufferPool.Get().(*bytes.Buffer)
	defer func() {
		buf.Reset()
		so.bufferPool.Put(buf)
	}()

	if _, err := io.Copy(buf, reader); err != nil {
		return nil, fmt.Errorf("failed to decompress data: %w", err)
	}

	return buf.Bytes(), nil
}

// Benchmark serialization performance
func (so *SerializationOptimizer) BenchmarkSerialization(customers []*pb.Customer) SerializationBenchmark {
	results := SerializationBenchmark{}

	// Test different approaches
	start := time.Now()
	
	// Individual serialization
	for _, customer := range customers {
		data, err := proto.Marshal(customer)
		if err == nil {
			results.IndividualSize += len(data)
		}
	}
	results.IndividualDuration = time.Since(start)

	// Batch serialization
	start = time.Now()
	batchData, err := so.MarshalBatchOptimized(customers, false)
	if err == nil {
		results.BatchSize = len(batchData)
	}
	results.BatchDuration = time.Since(start)

	// Compressed batch
	start = time.Now()
	compressedData, err := so.MarshalBatchOptimized(customers, true)
	if err == nil {
		results.CompressedSize = len(compressedData)
	}
	results.CompressedDuration = time.Since(start)

	return results
}

type SerializationBenchmark struct {
	IndividualSize     int
	IndividualDuration time.Duration
	BatchSize          int
	BatchDuration      time.Duration
	CompressedSize     int
	CompressedDuration time.Duration
}

func (sb SerializationBenchmark) Report() {
	fmt.Println("üì¶ Serialization Benchmark Results")
	fmt.Println("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
	
	fmt.Printf("Individual Messages:\n")
	fmt.Printf("   ‚Ä¢ Size: %d bytes\n", sb.IndividualSize)
	fmt.Printf("   ‚Ä¢ Duration: %v\n", sb.IndividualDuration)
	
	fmt.Printf("\nBatch Messages:\n")
	fmt.Printf("   ‚Ä¢ Size: %d bytes (%.1f%% of individual)\n", 
		sb.BatchSize, float64(sb.BatchSize)/float64(sb.IndividualSize)*100)
	fmt.Printf("   ‚Ä¢ Duration: %v\n", sb.BatchDuration)
	
	fmt.Printf("\nCompressed Batch:\n")
	fmt.Printf("   ‚Ä¢ Size: %d bytes (%.1f%% of batch)\n", 
		sb.CompressedSize, float64(sb.CompressedSize)/float64(sb.BatchSize)*100)
	fmt.Printf("   ‚Ä¢ Duration: %v\n", sb.CompressedDuration)
}
```

### Task 4: Caching Strategy Implementation (10 minutes)

Implement a comprehensive caching strategy.

**File: `internal/cache/redis_cache.go`**
```go
package cache

import (
	"context"
	"encoding/json"
	"fmt"
	"time"

	"github.com/go-redis/redis/v8"
	pb "chinook/proto/customer/v1"
)

type RedisCache struct {
	client *redis.Client
	ttl    time.Duration
}

func NewRedisCache(addr string, ttl time.Duration) *RedisCache {
	rdb := redis.NewClient(&redis.Options{
		Addr:         addr,
		Password:     "",
		DB:           0,
		PoolSize:     10,
		MinIdleConns: 5,
	})

	return &RedisCache{
		client: rdb,
		ttl:    ttl,
	}
}

func (rc *RedisCache) GetCustomer(ctx context.Context, id string) (*pb.Customer, error) {
	key := fmt.Sprintf("customer:%s", id)
	
	data, err := rc.client.Get(ctx, key).Result()
	if err != nil {
		if err == redis.Nil {
			return nil, nil // Cache miss
		}
		return nil, fmt.Errorf("redis get error: %w", err)
	}

	var customer pb.Customer
	if err := json.Unmarshal([]byte(data), &customer); err != nil {
		return nil, fmt.Errorf("failed to unmarshal customer: %w", err)
	}

	return &customer, nil
}

func (rc *RedisCache) SetCustomer(ctx context.Context, customer *pb.Customer) error {
	key := fmt.Sprintf("customer:%s", customer.Id)
	
	data, err := json.Marshal(customer)
	if err != nil {
		return fmt.Errorf("failed to marshal customer: %w", err)
	}

	err = rc.client.Set(ctx, key, data, rc.ttl).Err()
	if err != nil {
		return fmt.Errorf("redis set error: %w", err)
	}

	return nil
}

func (rc *RedisCache) InvalidateCustomer(ctx context.Context, id string) error {
	key := fmt.Sprintf("customer:%s", id)
	return rc.client.Del(ctx, key).Err()
}

func (rc *RedisCache) InvalidatePattern(ctx context.Context, pattern string) error {
	keys, err := rc.client.Keys(ctx, pattern).Result()
	if err != nil {
		return err
	}

	if len(keys) > 0 {
		return rc.client.Del(ctx, keys...).Err()
	}

	return nil
}
```

## üí• Deliberate Failure Points

Test these performance degradation scenarios:

1. **Memory Leaks**: Remove defer statements in object pools
2. **Connection Exhaustion**: Set MaxOpenConns to 1 and run load tests
3. **Cache Stampeding**: Remove cache locks and hit the same key simultaneously
4. **N+1 Queries**: Remove batch operations and query one-by-one

## ‚úÖ Victory Conditions

- [ ] CPU profiling shows no obvious hotspots
- [ ] Memory usage is stable under load (no leaks)
- [ ] Database queries use prepared statements and connection pooling
- [ ] Cache hit rates >80% for frequently accessed data
- [ ] Serialization performance improved by >50% with optimization

## üîç Further Exploration

- Implement distributed caching with consistent hashing
- Add query plan analysis for database optimization
- Create custom protobuf field options for performance hints
- Implement adaptive caching based on access patterns
- Add performance monitoring and alerting

Ready for the security and compliance lab? We'll secure your optimized pipeline! üîí