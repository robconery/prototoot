# Lab 4: 🎯 Final Pipeline Deployment

**Duration**: 60 minutes  
**Difficulty**: Advanced  

## 🎯 Objectives
- Deploy the complete Chinook data pipeline to production
- Set up comprehensive monitoring and observability
- Create production runbooks and documentation
- Validate end-to-end pipeline functionality

## 📋 Prerequisites
- Completed Labs 1-3 (testing, optimization, security)
- Docker and Kubernetes access
- AWS CLI configured (for S3 integration)

## 🛠️ Tasks

### Task 1: Production Deployment Configuration (20 minutes)

Create production-ready deployment configurations.

**File: `deployments/docker-compose.prod.yml`**
```yaml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    command: redis-server --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

  chinook-api:
    build:
      context: .
      dockerfile: Dockerfile.production
    environment:
      - DATABASE_URL=../resources/chinook.db
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
      - JWT_PRIVATE_KEY_PATH=/secrets/jwt-private.pem
      - JWT_PUBLIC_KEY_PATH=/secrets/jwt-public.pem
      - TLS_CERT_PATH=/secrets/tls.crt
      - TLS_KEY_PATH=/secrets/tls.key
      - LOG_LEVEL=info
      - ENVIRONMENT=production
    ports:
      - "8443:8443"
    volumes:
      - ./secrets:/secrets:ro
      - ./logs:/app/logs
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "grpc_health_probe", "-addr=:8443", "-tls", "-tls-no-verify"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G

  prometheus:
    image: prom/prometheus:latest
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources

volumes:
  redis_data:
  prometheus_data:
  grafana_data:
```

**File: `Dockerfile.production`**
```dockerfile
# Multi-stage build for production
FROM golang:1.21-alpine AS builder

# Install build dependencies
RUN apk add --no-cache git ca-certificates tzdata

WORKDIR /build

# Copy go mod files first for better caching
COPY go.mod go.sum ./
RUN go mod download

# Copy source code
COPY . .

# Build the application
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo \
    -ldflags '-w -s -extldflags "-static"' \
    -o chinook-api cmd/api/main.go

# Production stage
FROM scratch

# Import ca-certificates from builder
COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/
COPY --from=builder /usr/share/zoneinfo /usr/share/zoneinfo

# Copy the binary
COPY --from=builder /build/chinook-api /app/chinook-api

# Copy configuration
COPY --from=builder /build/configs /app/configs

# Create non-root user
USER 1000:1000

WORKDIR /app

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD ["/app/chinook-api", "healthcheck"]

EXPOSE 8443

CMD ["/app/chinook-api", "serve"]
```

**File: `kubernetes/namespace.yaml`**
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: chinook
  labels:
    name: chinook
    environment: production
```

**File: `kubernetes/configmap.yaml`**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: chinook-config
  namespace: chinook
data:
  app.yaml: |
    server:
      port: 8443
      tls:
        enabled: true
        cert_file: /secrets/tls.crt
        key_file: /secrets/tls.key
    
    database:
      file_path: ../resources/chinook.db
      max_open_conns: 25
      max_idle_conns: 5
      conn_max_lifetime: 30m
    
    cache:
      redis:
        addr: redis-service:6379
        db: 0
        pool_size: 10
        min_idle_conns: 5
    
    auth:
      jwt:
        issuer: chinook.com
        private_key_path: /secrets/jwt-private.pem
        public_key_path: /secrets/jwt-public.pem
        token_ttl: 24h
    
    logging:
      level: info
      format: json
      audit_log_path: /app/logs/audit.log
    
    metrics:
      enabled: true
      port: 9090
      path: /metrics
```

**File: `kubernetes/deployment.yaml`**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: chinook-api
  namespace: chinook
  labels:
    app: chinook-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: chinook-api
  template:
    metadata:
      labels:
        app: chinook-api
    spec:
      containers:
      - name: chinook-api
        image: chinook/api:latest
        ports:
        - containerPort: 8443
          name: grpc-tls
        - containerPort: 9090
          name: metrics
        env:
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: chinook-secrets
              key: database-password
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: chinook-secrets
              key: redis-password
        volumeMounts:
        - name: config
          mountPath: /app/configs
        - name: secrets
          mountPath: /secrets
          readOnly: true
        - name: logs
          mountPath: /app/logs
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2
            memory: 2Gi
        livenessProbe:
          exec:
            command:
            - /app/chinook-api
            - healthcheck
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          exec:
            command:
            - /app/chinook-api
            - healthcheck
          initialDelaySeconds: 5
          periodSeconds: 10
      volumes:
      - name: config
        configMap:
          name: chinook-config
      - name: secrets
        secret:
          secretName: chinook-secrets
      - name: logs
        emptyDir: {}
```

### Task 2: Monitoring & Observability (15 minutes)

Set up comprehensive monitoring for your production pipeline.

**File: `monitoring/prometheus.yml`**
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alerts.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'chinook-api'
    static_configs:
      - targets: ['chinook-api:9090']
    metrics_path: /metrics
    scrape_interval: 10s

  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
```

**File: `monitoring/alerts.yml`**
```yaml
groups:
  - name: chinook.rules
    rules:
      - alert: HighErrorRate
        expr: rate(grpc_server_handled_total{grpc_code!="OK"}[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors per second"

      - alert: DatabaseFileAccessible
        expr: up{job="chinook-api"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Database file not accessible"
          description: "SQLite database file may be inaccessible or corrupted"

      - alert: MemoryUsageHigh
        expr: process_resident_memory_bytes > 1e9
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage: {{ $value | humanize }}B"

      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service is down"
          description: "{{ $labels.job }} service is down"
```

**File: `internal/metrics/prometheus.go`**
```go
package metrics

import (
	"time"

	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promauto"
)

var (
	// Request metrics
	RequestsTotal = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "chinook_requests_total",
			Help: "Total number of requests processed",
		},
		[]string{"method", "status"},
	)

	RequestDuration = promauto.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "chinook_request_duration_seconds",
			Help:    "Request duration in seconds",
			Buckets: prometheus.DefBuckets,
		},
		[]string{"method"},
	)

	// Database metrics
	DatabaseConnections = promauto.NewGaugeVec(
		prometheus.GaugeOpts{
			Name: "chinook_database_connections",
			Help: "Number of database connections",
		},
		[]string{"state"},
	)

	DatabaseQueryDuration = promauto.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "chinook_database_query_duration_seconds",
			Help:    "Database query duration in seconds",
			Buckets: []float64{0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5},
		},
		[]string{"operation"},
	)

	// Cache metrics
	CacheHits = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "chinook_cache_hits_total",
			Help: "Total number of cache hits",
		},
		[]string{"cache"},
	)

	CacheMisses = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "chinook_cache_misses_total",
			Help: "Total number of cache misses",
		},
		[]string{"cache"},
	)

	// Business metrics
	CustomersCreated = promauto.NewCounter(
		prometheus.CounterOpts{
			Name: "chinook_customers_created_total",
			Help: "Total number of customers created",
		},
	)

	DataProcessingErrors = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "chinook_data_processing_errors_total",
			Help: "Total number of data processing errors",
		},
		[]string{"stage", "error_type"},
	)
)

// Middleware for measuring request metrics
func RequestMetricsMiddleware(method string) func() {
	timer := prometheus.NewTimer(RequestDuration.WithLabelValues(method))
	return func() {
		timer.ObserveDuration()
	}
}

// Record cache hit/miss
func RecordCacheResult(cache string, hit bool) {
	if hit {
		CacheHits.WithLabelValues(cache).Inc()
	} else {
		CacheMisses.WithLabelValues(cache).Inc()
	}
}

// Record database query
func RecordDatabaseQuery(operation string, duration time.Duration) {
	DatabaseQueryDuration.WithLabelValues(operation).Observe(duration.Seconds())
}
```

### Task 3: Production Runbooks (10 minutes)

Create operational runbooks for production support.

**File: `docs/runbooks/deployment.md`**
```markdown
# 🚀 Chinook Production Deployment Runbook

## Quick Start Deployment

### Prerequisites Checklist
- [ ] Kubernetes cluster ready (v1.25+)
- [ ] Docker images built and pushed
- [ ] Secrets configured (JWT keys, TLS certs, passwords)
- [ ] S3 bucket created and configured
- [ ] DNS records configured

### Deploy to Production

```bash
# 1. Create namespace
kubectl apply -f kubernetes/namespace.yaml

# 2. Create secrets
kubectl create secret generic chinook-secrets \
  --from-file=jwt-private.pem=secrets/jwt-private.pem \
  --from-file=jwt-public.pem=secrets/jwt-public.pem \
  --from-file=tls.crt=secrets/tls.crt \
  --from-file=tls.key=secrets/tls.key \
  --from-literal=database-password=$DB_PASSWORD \
  --from-literal=redis-password=$REDIS_PASSWORD \
  -n chinook

# 3. Deploy configuration
kubectl apply -f kubernetes/configmap.yaml

# 4. Deploy Redis
kubectl apply -f kubernetes/redis.yaml

# 6. Deploy application
kubectl apply -f kubernetes/deployment.yaml

# 7. Deploy services
kubectl apply -f kubernetes/services.yaml

# 8. Deploy ingress
kubectl apply -f kubernetes/ingress.yaml
```

### Health Checks

```bash
# Check pod status
kubectl get pods -n chinook

# Check service endpoints
kubectl get endpoints -n chinook

# Test API health
grpc_health_probe -addr=chinook.example.com:443 -tls

# Check logs
kubectl logs -f deployment/chinook-api -n chinook
```

### Rolling Updates

```bash
# Update application
kubectl set image deployment/chinook-api \
  chinook-api=chinook/api:v1.2.3 \
  -n chinook

# Monitor rollout
kubectl rollout status deployment/chinook-api -n chinook

# Rollback if needed
kubectl rollout undo deployment/chinook-api -n chinook
```

## Troubleshooting

### Common Issues

#### 1. Pods Not Starting
```bash
# Check events
kubectl describe pod <pod-name> -n chinook

# Common fixes:
# - Check resource limits
# - Verify secret mounts
# - Check image pull policy
```

#### 2. Database Connection Issues
```bash
# Test database connectivity
kubectl exec -it deployment/chinook-api -n chinook -- \
  ls -la ../resources/chinook.db

# Access SQLite database
kubectl exec -it deployment/chinook-api -n chinook -- \
  sqlite3 ../resources/chinook.db ".tables"
```

#### 3. Certificate Issues
```bash
# Verify TLS certificate
openssl x509 -in secrets/tls.crt -text -noout

# Test TLS connection
openssl s_client -connect chinook.example.com:443
```

### Performance Issues

#### High CPU Usage
```bash
# Check resource usage
kubectl top pods -n chinook

# Scale horizontally
kubectl scale deployment chinook-api --replicas=5 -n chinook
```

#### Memory Leaks
```bash
# Monitor memory usage
kubectl exec -it deployment/chinook-api -n chinook -- \
  curl localhost:9090/metrics | grep process_resident_memory_bytes

# Restart pods if needed
kubectl rollout restart deployment/chinook-api -n chinook
```

### Emergency Procedures

#### Complete Service Outage
1. Check cluster status: `kubectl cluster-info`
2. Check node status: `kubectl get nodes`
3. Check critical services: `kubectl get pods -n kube-system`
4. Scale to zero and back: `kubectl scale deployment chinook-api --replicas=0 -n chinook`
5. Check external dependencies (database, Redis, S3)

#### Data Corruption
1. Stop all write operations
2. Create database backup: `cp ../resources/chinook.db backup-chinook-$(date +%Y%m%d).db`
3. Analyze corruption scope
4. Restore from last known good backup
5. Replay audit logs for missing data

## Monitoring & Alerting

### Key Metrics to Watch
- Request rate and latency
- Error rate
- Database connection count
- Memory and CPU usage
- Cache hit rate

### Alert Response

#### Critical Alerts (Page immediately)
- Service completely down
- Error rate > 10%
- Database unavailable
- Security incident

#### Warning Alerts (Check within 1 hour)
- High resource usage
- Cache miss rate high
- Slow response times

### Grafana Dashboards
- Application Overview: http://grafana.example.com/d/chinook-overview
- Database Metrics: http://grafana.example.com/d/chinook-database
- Security Dashboard: http://grafana.example.com/d/chinook-security
```

### Task 4: End-to-End Validation (15 minutes)

Create comprehensive end-to-end tests to validate the complete pipeline.

**File: `tests/e2e/pipeline_test.go`**
```go
package e2e

import (
	"context"
	"crypto/tls"
	"fmt"
	"testing"
	"time"

	"google.golang.org/grpc"
	"google.golang.org/grpc/credentials"
	"google.golang.org/grpc/metadata"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	pb "chinook/proto/customer/v1"
	"chinook/internal/auth"
)

type E2ETestSuite struct {
	conn         *grpc.ClientConn
	client       pb.CustomerServiceClient
	auth         *auth.JWTAuthenticator
	adminToken   string
	readonlyToken string
}

func TestchinookPipeline(t *testing.T) {
	suite := setupE2ETests(t)
	defer suite.teardown()

	t.Run("Authentication", suite.testAuthentication)
	t.Run("Authorization", suite.testAuthorization)
	t.Run("CustomerCRUD", suite.testCustomerCRUD)
	t.Run("DataProcessing", suite.testDataProcessing)
	t.Run("Performance", suite.testPerformance)
	t.Run("Security", suite.testSecurity)
}

func setupE2ETests(t *testing.T) *E2ETestSuite {
	// Connect to production service
	config := &tls.Config{
		ServerName: "chinook.example.com",
	}
	creds := credentials.NewTLS(config)

	conn, err := grpc.Dial("chinook.example.com:443", grpc.WithTransportCredentials(creds))
	require.NoError(t, err)

	client := pb.NewCustomerServiceClient(conn)

	// Setup authentication
	authenticator, err := auth.NewJWTAuthenticator(
		loadPrivateKey(t), loadPublicKey(t), "chinook.com")
	require.NoError(t, err)

	// Generate test tokens
	adminToken, err := authenticator.GenerateToken(
		"test-admin", "admin@test.com", []auth.Role{auth.RoleAdmin})
	require.NoError(t, err)

	readonlyToken, err := authenticator.GenerateToken(
		"test-readonly", "readonly@test.com", []auth.Role{auth.RoleReadOnly})
	require.NoError(t, err)

	return &E2ETestSuite{
		conn:          conn,
		client:        client,
		auth:          authenticator,
		adminToken:    adminToken,
		readonlyToken: readonlyToken,
	}
}

func (suite *E2ETestSuite) testAuthentication(t *testing.T) {
	ctx := context.Background()

	t.Run("ValidTokenAccepted", func(t *testing.T) {
		ctx := suite.withAuth(ctx, suite.adminToken)
		
		_, err := suite.client.List(ctx, &pb.ListCustomersRequest{
			Limit:  10,
			Offset: 0,
		})
		assert.NoError(t, err, "Valid token should be accepted")
	})

	t.Run("InvalidTokenRejected", func(t *testing.T) {
		ctx := suite.withAuth(ctx, "invalid-token")
		
		_, err := suite.client.List(ctx, &pb.ListCustomersRequest{
			Limit:  10,
			Offset: 0,
		})
		assert.Error(t, err, "Invalid token should be rejected")
	})

	t.Run("MissingTokenRejected", func(t *testing.T) {
		_, err := suite.client.List(ctx, &pb.ListCustomersRequest{
			Limit:  10,
			Offset: 0,
		})
		assert.Error(t, err, "Missing token should be rejected")
	})
}

func (suite *E2ETestSuite) testAuthorization(t *testing.T) {
	ctx := context.Background()

	t.Run("AdminCanCreateCustomer", func(t *testing.T) {
		ctx := suite.withAuth(ctx, suite.adminToken)
		
		customer := &pb.Customer{
			Id:    fmt.Sprintf("e2e-admin-%d", time.Now().UnixNano()),
			Email: "admin-test@chinook.com",
			Name:  "Admin Test User",
			Phone: "+15551234567",
		}

		_, err := suite.client.Create(ctx, &pb.CreateCustomerRequest{
			Customer: customer,
		})
		assert.NoError(t, err, "Admin should be able to create customers")
	})

	t.Run("ReadonlyCannotCreateCustomer", func(t *testing.T) {
		ctx := suite.withAuth(ctx, suite.readonlyToken)
		
		customer := &pb.Customer{
			Id:    fmt.Sprintf("e2e-readonly-%d", time.Now().UnixNano()),
			Email: "readonly-test@chinook.com",
			Name:  "Readonly Test User",
			Phone: "+15551234567",
		}

		_, err := suite.client.Create(ctx, &pb.CreateCustomerRequest{
			Customer: customer,
		})
		assert.Error(t, err, "Readonly user should not be able to create customers")
	})

	t.Run("ReadonlyCanListCustomers", func(t *testing.T) {
		ctx := suite.withAuth(ctx, suite.readonlyToken)
		
		_, err := suite.client.List(ctx, &pb.ListCustomersRequest{
			Limit:  10,
			Offset: 0,
		})
		assert.NoError(t, err, "Readonly user should be able to list customers")
	})
}

func (suite *E2ETestSuite) testCustomerCRUD(t *testing.T) {
	ctx := suite.withAuth(context.Background(), suite.adminToken)
	
	customerID := fmt.Sprintf("e2e-crud-%d", time.Now().UnixNano())
	
	// Create
	originalCustomer := &pb.Customer{
		Id:    customerID,
		Email: "crud-test@chinook.com",
		Name:  "CRUD Test User",
		Phone: "+15551234567",
		Metadata: map[string]string{
			"test_type": "e2e",
			"created":   time.Now().Format(time.RFC3339),
		},
	}

	_, err := suite.client.Create(ctx, &pb.CreateCustomerRequest{
		Customer: originalCustomer,
	})
	require.NoError(t, err, "Should create customer successfully")

	// Read
	getResp, err := suite.client.Get(ctx, &pb.GetCustomerRequest{
		Id: customerID,
	})
	require.NoError(t, err, "Should retrieve customer successfully")
	assert.Equal(t, originalCustomer.Email, getResp.Customer.Email)

	// Update
	updatedCustomer := &pb.Customer{
		Id:    customerID,
		Email: "updated-crud-test@chinook.com",
		Name:  "Updated CRUD Test User",
		Phone: "+15559876543",
		Metadata: map[string]string{
			"test_type": "e2e",
			"updated":   time.Now().Format(time.RFC3339),
		},
	}

	_, err = suite.client.Update(ctx, &pb.UpdateCustomerRequest{
		Customer: updatedCustomer,
	})
	require.NoError(t, err, "Should update customer successfully")

	// Verify update
	getResp, err = suite.client.Get(ctx, &pb.GetCustomerRequest{
		Id: customerID,
	})
	require.NoError(t, err, "Should retrieve updated customer")
	assert.Equal(t, updatedCustomer.Email, getResp.Customer.Email)
	assert.Equal(t, updatedCustomer.Phone, getResp.Customer.Phone)

	// Delete
	_, err = suite.client.Delete(ctx, &pb.DeleteCustomerRequest{
		Id: customerID,
	})
	require.NoError(t, err, "Should delete customer successfully")

	// Verify deletion
	_, err = suite.client.Get(ctx, &pb.GetCustomerRequest{
		Id: customerID,
	})
	assert.Error(t, err, "Should not find deleted customer")
}

func (suite *E2ETestSuite) testDataProcessing(t *testing.T) {
	ctx := suite.withAuth(context.Background(), suite.adminToken)

	t.Run("BatchProcessing", func(t *testing.T) {
		// Create batch of customers
		customers := make([]*pb.Customer, 10)
		for i := range customers {
			customers[i] = &pb.Customer{
				Id:    fmt.Sprintf("e2e-batch-%d-%d", time.Now().UnixNano(), i),
				Email: fmt.Sprintf("batch%d@chinook.com", i),
				Name:  fmt.Sprintf("Batch User %d", i),
				Phone: fmt.Sprintf("+1555%07d", i),
			}
		}

		_, err := suite.client.BatchCreate(ctx, &pb.BatchCreateRequest{
			Customers: customers,
		})
		assert.NoError(t, err, "Batch creation should succeed")

		// Verify all customers were created
		for _, customer := range customers {
			_, err := suite.client.Get(ctx, &pb.GetCustomerRequest{
				Id: customer.Id,
			})
			assert.NoError(t, err, "Batch created customer should exist")
		}
	})

	t.Run("LargePayloadHandling", func(t *testing.T) {
		// Create customer with large metadata
		largeMetadata := make(map[string]string)
		for i := 0; i < 100; i++ {
			largeMetadata[fmt.Sprintf("key_%d", i)] = fmt.Sprintf("value_%d_with_some_longer_content_to_test_payload_size", i)
		}

		customer := &pb.Customer{
			Id:       fmt.Sprintf("e2e-large-%d", time.Now().UnixNano()),
			Email:    "large-payload@chinook.com",
			Name:     "Large Payload Test User",
			Phone:    "+15551234567",
			Metadata: largeMetadata,
		}

		_, err := suite.client.Create(ctx, &pb.CreateCustomerRequest{
			Customer: customer,
		})
		assert.NoError(t, err, "Large payload should be handled correctly")
	})
}

func (suite *E2ETestSuite) testPerformance(t *testing.T) {
	ctx := suite.withAuth(context.Background(), suite.adminToken)

	t.Run("ResponseTime", func(t *testing.T) {
		start := time.Now()
		
		_, err := suite.client.List(ctx, &pb.ListCustomersRequest{
			Limit:  100,
			Offset: 0,
		})
		
		duration := time.Since(start)
		assert.NoError(t, err, "List request should succeed")
		assert.Less(t, duration, 2*time.Second, "Response time should be under 2 seconds")
	})

	t.Run("ConcurrentRequests", func(t *testing.T) {
		const concurrency = 10
		results := make(chan error, concurrency)

		for i := 0; i < concurrency; i++ {
			go func(id int) {
				customer := &pb.Customer{
					Id:    fmt.Sprintf("e2e-concurrent-%d-%d", time.Now().UnixNano(), id),
					Email: fmt.Sprintf("concurrent%d@chinook.com", id),
					Name:  fmt.Sprintf("Concurrent User %d", id),
					Phone: fmt.Sprintf("+1555%07d", id),
				}

				_, err := suite.client.Create(ctx, &pb.CreateCustomerRequest{
					Customer: customer,
				})
				results <- err
			}(i)
		}

		// Collect results
		for i := 0; i < concurrency; i++ {
			err := <-results
			assert.NoError(t, err, "Concurrent request should succeed")
		}
	})
}

func (suite *E2ETestSuite) testSecurity(t *testing.T) {
	ctx := context.Background()

	t.Run("TLSConnection", func(t *testing.T) {
		// Connection should already be using TLS
		state, ok := peer.FromContext(ctx)
		if ok {
			if tlsInfo, ok := state.AuthInfo.(credentials.TLSInfo); ok {
				assert.Equal(t, tls.VersionTLS12, tlsInfo.State.Version, 
					"Should use TLS 1.2 or higher")
			}
		}
	})

	t.Run("InputValidation", func(t *testing.T) {
		ctx := suite.withAuth(ctx, suite.adminToken)

		// Test invalid email
		customer := &pb.Customer{
			Id:    "invalid-email-test",
			Email: "not-an-email",
			Name:  "Invalid Email Test",
			Phone: "+15551234567",
		}

		_, err := suite.client.Create(ctx, &pb.CreateCustomerRequest{
			Customer: customer,
		})
		assert.Error(t, err, "Invalid email should be rejected")
	})
}

func (suite *E2ETestSuite) withAuth(ctx context.Context, token string) context.Context {
	md := metadata.Pairs("authorization", "Bearer "+token)
	return metadata.NewOutgoingContext(ctx, md)
}

func (suite *E2ETestSuite) teardown() {
	if suite.conn != nil {
		suite.conn.Close()
	}
}

func loadPrivateKey(t *testing.T) []byte {
	// Load from test fixtures or environment
	// Implementation depends on your setup
	return []byte("mock-private-key")
}

func loadPublicKey(t *testing.T) []byte {
	// Load from test fixtures or environment
	// Implementation depends on your setup
	return []byte("mock-public-key")
}
```

**Run the complete end-to-end test:**
```bash
# Run e2e tests against production
go test -v ./tests/e2e/ -tags=e2e
```

## 💥 Deliberate Failure Points

Test these production scenarios:

1. **Deployment Failures**: Use invalid image tags and observe rollback
2. **Resource Exhaustion**: Set very low resource limits and watch pods fail
3. **Certificate Expiry**: Use expired certificates and test error handling
4. **Database Downtime**: Stop database during operations and test recovery

## ✅ Victory Conditions

- [ ] Complete pipeline deployed to production environment
- [ ] All services healthy and responding correctly
- [ ] Monitoring dashboard showing green metrics
- [ ] End-to-end tests passing in production
- [ ] Security scan showing no critical vulnerabilities
- [ ] Load tests meeting performance targets
- [ ] Runbooks complete and tested

## 🔍 Further Exploration

- Implement blue-green deployments
- Add automated disaster recovery procedures
- Create multi-region deployment strategy
- Implement advanced observability with distributed tracing
- Add automated performance regression testing

## 🎉 Congratulations!

You've successfully built and deployed a production-ready Chinook data pipeline! You now have:

- **🏗️ Solid Foundation**: Go project with proper structure and idioms
- **📦 Protobuf Mastery**: Schema design and evolution strategies
- **🔄 Stream Processing**: Event-driven architecture with Bufstream
- **🔒 Security**: Authentication, authorization, and encryption
- **📊 Observability**: Comprehensive monitoring and alerting
- **🧪 Quality**: Full test coverage and automated validation
- **🚀 Production Ready**: Deployed with proper DevOps practices

You're now ready to advocate for Buf and build amazing data systems! 🎯✨
